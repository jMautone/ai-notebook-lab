{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rag-lab-title"
      },
      "source": [
        "# Laboratorio 3: Implementación de pipeline RAG avanzado\n",
        "\n",
        "En este laboratorio implementarás un pipeline RAG (Retrieval-Augmented Generation) completo utilizando LangChain y Pinecone. Explorarás técnicas avanzadas de preprocesamiento, chunking, retrievers y optimización de contextos.\n",
        "\n",
        "**Objetivos de aprendizaje:**\n",
        "- Construir un pipeline RAG básico con LangChain.\n",
        "- Implementar técnicas de preprocesamiento de texto.\n",
        "- Explorar diferentes estrategias de chunking.\n",
        "- Utilizar retrievers avanzados y filtros de metadata.\n",
        "- Aplicar reordenamiento de contextos para evitar \"Lost in the Middle\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "## Parte 1: Setup y pipeline RAG básico\n",
        "\n",
        "Comenzaremos configurando el entorno y creando un pipeline RAG básico que servirá como base para las mejoras posteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Instalar dependencias\n",
        "%pip install langchain langchain-community langchain-pinecone sentence-transformers spacy langchain-huggingface pypdf\n",
        "\n",
        "# Descargar modelo de español para preprocesamiento\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "from langchain.schema import Document\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_community.document_transformers import LongContextReorder\n",
        "import spacy\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDiEA5ZkhcgR"
      },
      "source": [
        "Utiliza la clase `PyPDFLoader` del módulo `langchain_community.document_loaders` para cargar un archivo PDF.\n",
        "\n",
        "Guarda el contenido del documento en una variable llamada `docs`.\n",
        "\n",
        "Muestra por consola:\n",
        "- La cantidad de documentos cargados desde el PDF.\n",
        "- Los primeros 200 caracteres del contenido de la primera página."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvUVPC2wULJ2"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Subir el archivo PDF desde tu computadora\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Obtener el nombre del archivo subido\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "print(f\"Archivo subido: {pdf_path}\")\n",
        "\n",
        "# Cargar el PDF con PyPDFLoader\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "# Mostrar la cantidad de documentos cargados\n",
        "print(f\"Cantidad de documentos cargados: {len(docs)}\")\n",
        "\n",
        "# Mostrar los primeros 200 caracteres del contenido de la primera página\n",
        "if docs:\n",
        "    print(\"\\nPrimeros 200 caracteres de la primera página:\\n\")\n",
        "    print(docs[0].page_content[:200])\n",
        "else:\n",
        "    print(\"No se cargaron documentos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naBDD2VVjg6l"
      },
      "source": [
        "Crea un modelo de lenguaje (LLM) utilizando `HuggingFaceEndpoint`, con el identificador de modelo `\"Qwen/Qwen3-4B-Instruct-2507\"`.\n",
        "\n",
        "Parámetros a utilizar:\n",
        "- `task=\"text-generation\"`\n",
        "- `max_new_tokens=512`\n",
        "- `do_sample=False`\n",
        "- `repetition_penalty=1.03`\n",
        "- `provider=\"auto\"`\n",
        "\n",
        "Inicializa un objeto `ChatHuggingFace` a partir del LLM creado.\n",
        "\n",
        "Aplica una división del contenido previamente cargado en `docs` mediante `RecursiveCharacterTextSplitter`, con:\n",
        "- `chunk_size=500`\n",
        "- `chunk_overlap=50`\n",
        "\n",
        "Finalmente, muestra por pantalla:\n",
        "- La cantidad de documentos originales.\n",
        "- La cantidad de chunks generados.\n",
        "- El tamaño promedio de los chunks (en caracteres)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "basic-rag"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 1) Crear el LLM con HuggingFaceEndpoint\n",
        "hf_token = userdata.get('HUGGING_API_KEY')\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        "    provider=\"auto\",\n",
        "    huggingfacehub_api_token=hf_token,\n",
        ")\n",
        "\n",
        "# 2) Inicializar ChatHuggingFace a partir del LLM\n",
        "chat = ChatHuggingFace(llm=llm)\n",
        "\n",
        "# 3) Chunking con RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "# 4) Métricas pedidas\n",
        "num_docs = len(docs)\n",
        "num_chunks = len(chunks)\n",
        "avg_chunk_size = (sum(len(c.page_content) for c in chunks) / num_chunks) if num_chunks else 0\n",
        "\n",
        "print(f\"Cantidad de documentos originales: {num_docs}\")\n",
        "print(f\"Cantidad de chunks generados: {num_chunks}\")\n",
        "print(f\"Tamaño promedio de los chunks (caracteres): {avg_chunk_size:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w_45oKMl0LW"
      },
      "source": [
        "Crea un índice en Pinecone con las siguientes características:\n",
        "- **Dimensión**: 384\n",
        "- **Métrica**: \"cosine\"\n",
        "- **Especificación del servidor**: `ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")`\n",
        "\n",
        "Configura un modelo de embeddings con `HuggingFaceEmbeddings`, utilizando `\"sentence-transformers/all-MiniLM-L6-v2\"`.\n",
        "\n",
        "Crea un vector store de tipo `PineconeVectorStore` a partir de los chunks previamente generados, usando los embeddings definidos y el índice creado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-vectorstore"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "import os\n",
        "from pinecone import ServerlessSpec\n",
        "from google.colab import userdata\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "# === API Key desde secreto de Colab ===\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY  # para LangChain/PineconeVectorStore\n",
        "\n",
        "# === 1) Crear índice serverless en Pinecone ===\n",
        "index_name = \"rag-lab-index\"\n",
        "\n",
        "existing = [it[\"name\"] for it in pc.list_indexes().indexes]\n",
        "if index_name not in existing:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "# Opcional: obtener el handle del índice\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# === 2) Embeddings de HuggingFace ===\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# === 3) Crear el Vector Store en Pinecone a partir de los chunks ===\n",
        "# Puedes usar el handle del índice existente...\n",
        "# vectorstore = PineconeVectorStore(index=index, embedding=embeddings)\n",
        "# vectorstore.add_documents(chunks)\n",
        "\n",
        "# ...o crear directamente desde los documentos (usa el índice ya creado):\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    index_name=index_name,\n",
        ")\n",
        "\n",
        "print(f\"Índice '{index_name}' listo y vector store creado con {len(chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrjd_K-_nyRm"
      },
      "source": [
        "Define e inicializa una variable con una pregunta sobre el documento cargado.\n",
        "\n",
        "Utiliza el método `as_retriever(k=3)` del objeto `vectorstore` para obtener los 3 documentos más relevantes respecto a la pregunta.\n",
        "\n",
        "Define e inicializa una variable con el contexto (resultados de la consulta a la base de datos vectorial)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64c356d1"
      },
      "outputs": [],
      "source": [
        "# 1) Definir la pregunta\n",
        "question = \"Nombrame las vulnerabilidades críticas que se nombran en el proyecto.\"\n",
        "#question = \"Cuales son los problemas de LogicaORT?\"\n",
        "\n",
        "# 2) Obtener los 3 documentos más relevantes\n",
        "retriever = vectorstore.as_retriever(k=3)\n",
        "relevant_docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "# 3) Construir el contexto uniendo el contenido de los documentos\n",
        "context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "# 4) Crear y mostrar el prompt formateado\n",
        "prompt_template = \"\"\"Using the following context, answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "formatted_prompt = prompt_template.format(context=context, question=question)\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeIXvv07iITe"
      },
      "source": [
        "Invoca el modelo `chat_model` pasando como entrada el `formatted_prompt` (que incluye el contexto recuperado) y guarda la respuesta.\n",
        "\n",
        "Imprime en pantalla la pregunta original y la respuesta generada con contexto.\n",
        "\n",
        "Luego, invoca nuevamente el modelo con la misma pregunta, pero sin contexto, guardando el resultado en `without_context`.\n",
        "\n",
        "Imprime la respuesta sin contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3CmARhWN_5M"
      },
      "outputs": [],
      "source": [
        "# 1) Invocar el modelo con contexto\n",
        "with_context = chat.invoke(formatted_prompt)\n",
        "\n",
        "# 2) Mostrar pregunta y respuesta con contexto\n",
        "print(\"=== Pregunta ===\")\n",
        "print(question)\n",
        "print(\"\\n=== Respuesta con contexto ===\")\n",
        "print(with_context.content)\n",
        "\n",
        "# 3) Invocar el modelo con la misma pregunta pero sin contexto\n",
        "without_context = chat.invoke(question)\n",
        "\n",
        "# 4) Mostrar la respuesta sin contexto\n",
        "print(\"\\n=== Respuesta sin contexto ===\")\n",
        "print(without_context.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing-section"
      },
      "source": [
        "## Parte 2: Preprocesamiento de texto y chunking\n",
        "\n",
        "### Preprocesamiento de texto\n",
        "\n",
        "El preprocesamiento de texto es crucial para mejorar la calidad de los embeddings. Según las mejores prácticas, implementaremos:\n",
        "\n",
        "1. **Lematización**: Reducir palabras a su forma raíz.\n",
        "2. **Expansión de abreviaturas**: Convertir abreviaciones a formas completas.\n",
        "3. **Eliminación de stopwords**: Remover palabras sin significado semántico.\n",
        "4. **Normalización**: Limpiar caracteres especiales y espacios.\n",
        "\n",
        "Estas técnicas mejoran la capacidad de coincidencia semántica y reducen el ruido en los embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-spacy"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(\"Modelo de inglés cargado exitosamente\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocessing-functions"
      },
      "outputs": [],
      "source": [
        "def prepare_text_for_embeddings(content_str):\n",
        "    \"\"\"Preprocesar texto para mejorar calidad de embeddings\"\"\"\n",
        "    if content_str is None or content_str.strip() == \"\":\n",
        "        return \"N/A\"\n",
        "\n",
        "    content_str = re.sub(r'\\W', ' ', content_str)\n",
        "\n",
        "    doc = nlp(content_str.lower())\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
        "\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "    return lemmatized_text if lemmatized_text.strip() else \"N/A\"\n",
        "\n",
        "def expand_abbreviations(text):\n",
        "    \"\"\"Expandir abreviaturas comunes\"\"\"\n",
        "    if text is None or text.strip() == \"\":\n",
        "        return \"N/A\"\n",
        "\n",
        "    abbreviations = {\n",
        "        \"LLM\": \"Large Language Model\",\n",
        "        \"RAG\": \"Retrieval-Augmented Generation\",\n",
        "        \"SLM\": \"Small Language Model\",\n",
        "        \"HNSW\": \"Hierarchical Navigable Small Worlds\",\n",
        "        \"MMR\": \"Maximal Marginal Relevance\",\n",
        "        \"BERT\": \"Bidirectional Encoder Representations from Transformers\",\n",
        "        \"GPT\": \"Generative Pre-trained Transformer\",\n",
        "        \"NLP\": \"Natural Language Processing\",\n",
        "        \"ML\": \"Machine Learning\",\n",
        "        \"AI\": \"Artificial Intelligence\"\n",
        "    }\n",
        "    for abbr, full in abbreviations.items():\n",
        "        text = re.sub(r'\\b{}\\b'.format(abbr), full, text)\n",
        "    return text\n",
        "\n",
        "def normalize_text(s, sep_token = \" \\n \"):\n",
        "    \"\"\"Normalizar texto para consistencia\"\"\"\n",
        "    if s is None or s.strip() == \"\":\n",
        "        return \"N/A\"\n",
        "\n",
        "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
        "    s = re.sub(r\". ,\",\"\",s)\n",
        "    s = s.replace(\"..\",\".\")\n",
        "    s = s.replace(\". .\",\".\")\n",
        "    s = s.replace(\"\\n\", \"\")\n",
        "    s = s.strip()\n",
        "\n",
        "    if not s:\n",
        "        return \"N/A\"\n",
        "    return s\n",
        "\n",
        "print(\"Funciones de preprocesamiento definidas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zwAtv3FRpLf"
      },
      "outputs": [],
      "source": [
        "processed_chunks = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    # Get the page_content from the chunk\n",
        "    original_text = chunk.page_content\n",
        "\n",
        "    # Apply ALL preprocessing steps\n",
        "    processed_text = prepare_text_for_embeddings(\n",
        "        normalize_text(\n",
        "            expand_abbreviations(original_text)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Create a new Document with processed content\n",
        "    processed_chunk = Document(\n",
        "        page_content=processed_text,\n",
        "        metadata=chunk.metadata  # Keep original metadata\n",
        "    )\n",
        "    processed_chunks.append(processed_chunk)\n",
        "\n",
        "    if i < 2:\n",
        "        print(f\"\\nChunk {i+1}:\")\n",
        "        print(f\"Original: {original_text[:100]}...\")\n",
        "        print(f\"Procesado: {processed_text[:100]}...\")\n",
        "\n",
        "print(f\"\\nProcesamiento completado: {len(processed_chunks)} chunks procesados\")\n",
        "\n",
        "# Show the text improvement\n",
        "print(\"=== COMPARACIÓN DE CALIDAD DE TEXTO ===\")\n",
        "for i in range(min(3, len(chunks))):\n",
        "    original = chunks[i].page_content\n",
        "    processed = processed_chunks[i].page_content\n",
        "\n",
        "    print(f\"\\nChunk {i+1}:\")\n",
        "    print(f\"Original: {original[:150]}...\")\n",
        "    print(f\"Procesado: {processed[:150]}...\")\n",
        "\n",
        "    # Check for actual improvements\n",
        "    improvements = []\n",
        "    if len(processed) < len(original):\n",
        "        improvements.append(\"✓ Stopwords removidas\")\n",
        "    if processed != original:\n",
        "        improvements.append(\"✓ Lemmatización aplicada\")\n",
        "    if \"Large Language Model\" in processed and \"LLM\" in original:\n",
        "        improvements.append(\"✓ Abreviaciones expandidas\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chunking-strategies"
      },
      "source": [
        "### Estrategias de chunking\n",
        "\n",
        "El chunking es uno de los pasos más críticos en RAG. Diferentes estrategias tienen diferentes ventajas:\n",
        "\n",
        "1. **Chunks pequeños (200-300 caracteres)**: Más precisos, mejor para preguntas específicas.\n",
        "2. **Chunks medianos (500-800 caracteres)**: Balance entre precisión y contexto.\n",
        "3. **Chunks grandes (1000+ caracteres)**: Más contexto, mejor para preguntas complejas y estructuras con tablas, pero mayores costos y se alcanza más rápido la ventana de contexto.\n",
        "\n",
        "El overlap es crucial para evitar pérdida de información en los límites de los chunks. Un overlap del 10-20% del tamaño del chunk suele ser óptimo.\n",
        "\n",
        "> El overlap es el área de texto compartida entre dos fragmentos consecutivos. Es la porción de texto del final de un chunk ($N$) que se repite al comienzo del chunk siguiente ($N+1$).\n",
        "\n",
        "**Consideraciones importantes:**\n",
        "- Chunks muy pequeños pueden perder contexto importante.\n",
        "- Chunks muy grandes pueden causar \"Lost in the Middle\":\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "- El tamaño óptimo depende del tipo de consultas esperadas.\n",
        "- Para tablas largas, considerar Parent Document Retriever.\n",
        "- Para documentos cortos (como CVs), no chunking puede ser mejor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6B0ElMhlEx4"
      },
      "source": [
        "Escribe un programa que permita comparar distintas estrategias de chunking (para chunks pequeños, medianos y grandes) utilizando `RecursiveCharacterTextSplitter` de LangChain.\n",
        "\n",
        "Para cada estrategia del diccionario:\n",
        "- Aplica el método `split_documents()` sobre la colección `processed_chunks`.\n",
        "- Calcula y almacena en un diccionario llamado `chunking_results`:\n",
        "  - La cantidad de chunks generados (`chunk_count`).\n",
        "  - El tamaño promedio de los chunks (`avg_chunk_size`).\n",
        "  - La lista completa de chunks resultante.\n",
        "- Imprime en pantalla el nombre de la estrategia, el número total de chunks y su tamaño promedio en caracteres.\n",
        "\n",
        "Finalmente, muestra un ejemplo de salida correspondiente a la estrategia de chunks medianos, imprimiendo los primeros dos chunks y los 150 primeros caracteres de cada uno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chunking-comparison"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Definir distintas estrategias de chunking\n",
        "chunking_strategies = {\n",
        "    \"pequeños\": {\"chunk_size\": 300, \"chunk_overlap\": 30},\n",
        "    \"medianos\": {\"chunk_size\": 600, \"chunk_overlap\": 60},\n",
        "    \"grandes\":  {\"chunk_size\": 1000, \"chunk_overlap\": 100},\n",
        "}\n",
        "\n",
        "# Diccionario para almacenar resultados\n",
        "chunking_results = {}\n",
        "\n",
        "# Aplicar cada estrategia\n",
        "for name, params in chunking_strategies.items():\n",
        "    splitter = RecursiveCharacterTextSplitter(**params)\n",
        "    split_chunks = splitter.split_documents(docs)\n",
        "\n",
        "    # Calcular métricas\n",
        "    chunk_count = len(split_chunks)\n",
        "    avg_chunk_size = sum(len(c.page_content) for c in split_chunks) / chunk_count if chunk_count else 0\n",
        "\n",
        "    # Guardar resultados\n",
        "    chunking_results[name] = {\n",
        "        \"chunk_count\": chunk_count,\n",
        "        \"avg_chunk_size\": avg_chunk_size,\n",
        "        \"chunks\": split_chunks\n",
        "    }\n",
        "\n",
        "    # Mostrar resumen por estrategia\n",
        "    print(f\"Estrategia: {name}\")\n",
        "    print(f\" - Cantidad de chunks: {chunk_count}\")\n",
        "    print(f\" - Tamaño promedio de chunks: {avg_chunk_size:.2f} caracteres\\n\")\n",
        "\n",
        "# Mostrar ejemplo para la estrategia de chunks medianos\n",
        "print(\"=== Ejemplo de chunks (estrategia: medianos) ===\")\n",
        "med_chunks = chunking_results[\"medianos\"][\"chunks\"]\n",
        "\n",
        "for i, chunk in enumerate(med_chunks[:2]):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\")\n",
        "    print(chunk.page_content[:150])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "metadata-section"
      },
      "source": [
        "## Parte 3: Metadata y retrievers avanzados\n",
        "\n",
        "### Agregado de metadata\n",
        "\n",
        "La metadata es fundamental para mejorar la precisión de las búsquedas. Permite:\n",
        "\n",
        "1. **Filtrado por tema**: Buscar solo en documentos relevantes.\n",
        "2. **Filtrado por dificultad**: Adaptar respuestas al nivel del usuario.\n",
        "3. **Trazabilidad**: Mostrar fuente original del documento.\n",
        "4. **Navegación**: Enlazar al documento original.\n",
        "\n",
        "**Ejemplos de metadata útiles:**\n",
        "- `source`: Archivo original\n",
        "- `page`: Número de página\n",
        "- `section`: Sección del documento\n",
        "- `topic`: Tema principal\n",
        "- `difficulty`: Nivel de complejidad\n",
        "- `timestamp`: Fecha de creación/modificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "add-metadata"
      },
      "outputs": [],
      "source": [
        "documents_with_metadata = []\n",
        "for i, chunk in enumerate(processed_chunks):\n",
        "    # Determinar tema basado en contenido\n",
        "    content = chunk.page_content.lower()\n",
        "    if \"chunking\" in content or \"splitter\" in content:\n",
        "        topic = \"chunking\"\n",
        "    elif \"retriever\" in content or \"búsqueda\" in content or \"search\" in content:\n",
        "        topic = \"retrieval\"\n",
        "    elif \"vector\" in content or \"embedding\" in content:\n",
        "        topic = \"vector_db\"\n",
        "    elif \"contexto\" in content or \"reorder\" in content or \"attention\" in content:\n",
        "        topic = \"context\"\n",
        "    elif \"transformer\" in content or \"neural\" in content:\n",
        "        topic = \"neural_networks\"\n",
        "    elif \"machine\" in content or \"learning\" in content:\n",
        "        topic = \"machine_learning\"\n",
        "    else:\n",
        "        topic = \"general\"\n",
        "\n",
        "    # Flatten metadata - only keep simple values\n",
        "    doc_with_meta = Document(\n",
        "        page_content=chunk.page_content,\n",
        "        metadata={\n",
        "            \"source\": f\"transformer_paper_{i}\",\n",
        "            \"topic\": topic,\n",
        "            \"difficulty\": [\"beginner\", \"intermediate\", \"advanced\"][i % 3],\n",
        "            \"length\": len(chunk.page_content),\n",
        "            \"doc_id\": i,\n",
        "            \"page\": chunk.metadata.get(\"page\", i),\n",
        "            \"pdf_source\": chunk.metadata.get(\"source\", \"\"),\n",
        "            \"creator\": chunk.metadata.get(\"creator\", \"\"),\n",
        "            \"creation_date\": chunk.metadata.get(\"creationdate\", \"\")\n",
        "        }\n",
        "    )\n",
        "    documents_with_metadata.append(doc_with_meta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9L7PqVkmjzX"
      },
      "source": [
        "Crea un nuevo índice en Pinecone y guarda los documentos con metadata generados anteriormente (`documents_with_metadata`). Recuerda utilizar `PineconeVectorStore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-khiMZaeUHbG"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone\n",
        "import os\n",
        "\n",
        "# 1) Conexión a Pinecone usando la API Key guardada en secreto\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "# 2) Crear un nuevo índice para documentos con metadata\n",
        "index_name_meta = \"rag-lab-index-metadata\"\n",
        "\n",
        "existing_indexes = [it[\"name\"] for it in pc.list_indexes().indexes]\n",
        "if index_name_meta not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name_meta,\n",
        "        dimension=384,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "# 3) Configurar modelo de embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# 4) Crear el nuevo vector store con metadata\n",
        "vectorstore_meta = PineconeVectorStore.from_documents(\n",
        "    documents=documents_with_metadata,\n",
        "    embedding=embeddings,\n",
        "    index_name=index_name_meta,\n",
        ")\n",
        "\n",
        "print(f\"Índice '{index_name_meta}' creado con {len(documents_with_metadata)} documentos con metadata.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nma4hB-WnNL1"
      },
      "source": [
        "Realiza una búsqueda sin filtro (`similarity_search`) recuperando los 3 documentos más relevantes (`k=3`).\n",
        "- Imprime cada resultado mostrando su número, el valor del campo `topic` y el campo `difficulty` contenidos en los metadatos de cada documento.\n",
        "\n",
        "Realiza luego una búsqueda filtrada (`similarity_search`) aplicando un filtro que limite los resultados a documentos cuyo `topic` sea `\"neural_networks\"`.\n",
        "- Muestra nuevamente los resultados con el mismo formato que la búsqueda anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hsuq_jnVP9w"
      },
      "outputs": [],
      "source": [
        "# 1) Búsqueda sin filtro: recupera los 3 documentos más relevantes\n",
        "query = \"Nombrame las vulnerabilidades críticas documentadas en el reporte de pruebas de penetración.\"\n",
        "results = vectorstore_meta.similarity_search(query, k=3)\n",
        "\n",
        "print(\"=== Resultados sin filtro (k=3) ===\")\n",
        "if not results:\n",
        "    print(\"No se encontraron resultados.\")\n",
        "else:\n",
        "    for i, doc in enumerate(results, start=1):\n",
        "        topic = doc.metadata.get(\"topic\", \"N/A\")\n",
        "        difficulty = doc.metadata.get(\"difficulty\", \"N/A\")\n",
        "        print(f\"{i}) topic={topic} | difficulty={difficulty}\")\n",
        "\n",
        "# 2) Búsqueda filtrada: solo documentos con topic = 'neural_networks'\n",
        "filtered_results = vectorstore_meta.similarity_search(\n",
        "    query,\n",
        "    k=3,\n",
        "    filter={\"topic\": \"neural_networks\"}\n",
        ")\n",
        "\n",
        "print(\"\\n=== Resultados con filtro (topic='neural_networks', k=3) ===\")\n",
        "if not filtered_results:\n",
        "    print(\"No se encontraron resultados con ese filtro.\")\n",
        "else:\n",
        "    for i, doc in enumerate(filtered_results, start=1):\n",
        "        topic = doc.metadata.get(\"topic\", \"N/A\")\n",
        "        difficulty = doc.metadata.get(\"difficulty\", \"N/A\")\n",
        "        print(f\"{i}) topic={topic} | difficulty={difficulty}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reordering-section"
      },
      "source": [
        "## Parte 4: Reordenamiento de contextos y optimización\n",
        "\n",
        "### El problema \"Lost in the Middle\"\n",
        "\n",
        "Los modelos de lenguaje basados en transformers tienen una limitación conocida: prestan más atención al inicio y final del contexto, perdiendo información en el medio. Esto es especialmente problemático en RAG cuando se recuperan múltiples documentos.\n",
        "\n",
        "**Solución: Long-Context Reorder**\n",
        "\n",
        "Esta técnica reordena los documentos recuperados para optimizar la atención del modelo:\n",
        "1. **Documento más relevante**: Al inicio.\n",
        "2. **Documentos menos relevantes**: Al final.\n",
        "3. **Documentos de relevancia media**: En el medio.\n",
        "\n",
        "Esto mejora significativamente la calidad de las respuestas, especialmente con contextos largos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtTPuztZo-l-"
      },
      "source": [
        "En el siguiente bloque, completa el código para cumplir con la siguiente consigna:\n",
        "\n",
        "Muestra por pantalla:\n",
        "- La consulta realizada.\n",
        "- El orden original de los documentos (indicando su número, el valor de `topic` en sus metadatos y los primeros 60 caracteres de su contenido).\n",
        "\n",
        "Aplica el método `transform_documents()` del objeto `reordering` sobre los resultados obtenidos para reordenar los documentos.\n",
        "\n",
        "Imprime el nuevo orden de los documentos tras el reordenamiento, con el mismo formato que antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "implement-reordering"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_transformers import LongContextReorder\n",
        "\n",
        "# Alias por si tu variable se llama vectorstore_meta\n",
        "vectorstore_with_metadata = vectorstore_meta\n",
        "\n",
        "reordering = LongContextReorder()\n",
        "\n",
        "query = \"Nombrame las vulnerabilidades críticas documentadas en el reporte de pruebas de penetración.\"\n",
        "results = vectorstore_with_metadata.similarity_search(query, k=5)\n",
        "\n",
        "# Mostrar consulta\n",
        "print(\"=== Consulta ===\")\n",
        "print(query)\n",
        "\n",
        "# Mostrar orden original\n",
        "print(\"\\n=== Orden original de documentos ===\")\n",
        "if not results:\n",
        "    print(\"No se recuperaron documentos.\")\n",
        "else:\n",
        "    for i, doc in enumerate(results, start=1):\n",
        "        topic = doc.metadata.get(\"topic\", \"N/A\")\n",
        "        preview = (doc.page_content or \"\")[:60].replace(\"\\n\", \" \")\n",
        "        print(f\"{i}) topic={topic} | preview=\\\"{preview}\\\"\")\n",
        "\n",
        "# Reordenar documentos con LongContextReorder\n",
        "reordered = reordering.transform_documents(results)\n",
        "\n",
        "# Mostrar nuevo orden\n",
        "print(\"\\n=== Nuevo orden tras reordenamiento (Long-Context Reorder) ===\")\n",
        "if not reordered:\n",
        "    print(\"No hay documentos para reordenar.\")\n",
        "else:\n",
        "    for i, doc in enumerate(reordered, start=1):\n",
        "        topic = doc.metadata.get(\"topic\", \"N/A\")\n",
        "        preview = (doc.page_content or \"\")[:60].replace(\"\\n\", \" \")\n",
        "        print(f\"{i}) topic={topic} | preview=\\\"{preview}\\\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optimization-section"
      },
      "source": [
        "### Optimización de parámetros\n",
        "\n",
        "**Parámetro K (Número de documentos recuperados)**\n",
        "\n",
        "El parámetro K es crucial para el rendimiento:\n",
        "- **K bajo (1-3)**: Mayor precisión, menos contexto.\n",
        "- **K medio (3-5)**: Balance entre precisión y cobertura.\n",
        "- **K alto (5-10)**: Mayor cobertura, posible ruido.\n",
        "\n",
        "**Técnicas de reranking**\n",
        "\n",
        "1. **MMR (Maximal Marginal Relevance)**: Diversifica resultados.\n",
        "2. **Reranking basado en relevancia**: Prioriza documentos más relevantes.\n",
        "3. **Threshold vs. Top-K**: Usar umbral de similitud o número fijo.\n",
        "\n",
        "**Estrategias de optimización:**\n",
        "- Probar diferentes valores de K.\n",
        "- Evaluar métricas de precisión y recall.\n",
        "- Considerar el tipo de consultas esperadas.\n",
        "- Balancear velocidad vs. calidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "parameter-optimization"
      },
      "outputs": [],
      "source": [
        "# Test K values and MMR reranking\n",
        "test_query = \"What is attention mechanism?\"\n",
        "print(f\"Query: {test_query}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Test K values (precision vs recall)\n",
        "print(\"1. K VALUES:\")\n",
        "k_values = [1, 3, 5]\n",
        "for k in k_values:\n",
        "    docs = vectorstore_with_metadata.similarity_search(test_query, k=k)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {test_query}\\n\\nAnswer:\"\n",
        "    response = chat.invoke(prompt)\n",
        "    print(f\"  K={k}: {len(str(response))} chars\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# 2. Test MMR reranking (diversification)\n",
        "print(\"2. MMR RERANKING:\")\n",
        "mmr_results = vectorstore_with_metadata.similarity_search_with_relevance_scores(\n",
        "    test_query,\n",
        "    k=5,\n",
        "    fetch_k=10  # More candidates for MMR\n",
        ")\n",
        "print(f\"  MMR: {len(mmr_results)} docs with scores\")\n",
        "for i, (doc, score) in enumerate(mmr_results):\n",
        "    print(f\"    {i+1}. Score: {score:.3f} - {doc.metadata['topic']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SUMMARY:\")\n",
        "print(\"K=1: High precision, low recall\")\n",
        "print(\"K=5: Balanced precision/recall\")\n",
        "print(\"MMR: Diversifies results, reduces redundancy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusions"
      },
      "source": [
        "## Conclusiones y mejores prácticas\n",
        "\n",
        "### Resumen de técnicas implementadas\n",
        "\n",
        "1. **Preprocesamiento de texto**: Lemmatización, expansión de abreviaturas y normalización mejoran la calidad de los embeddings.\n",
        "\n",
        "2. **Estrategias de chunking**: El tamaño óptimo depende del caso de uso (balancear precisión vs. contexto).\n",
        "\n",
        "3. **Metadata y filtros**: Agregar metadata temática y de dificultad mejora la relevancia de las búsquedas.\n",
        "\n",
        "4. **Retrievers avanzados**: Ensemble retrievers combinan búsqueda densa y semántica para mejor cobertura.\n",
        "\n",
        "5. **Reordenamiento de contextos**: LongContextReorder evita el fenómeno \"Lost in the Middle\".\n",
        "\n",
        "6. **Optimización de parámetros**: Ajustar K y usar técnicas de reranking mejora la calidad de las respuestas.\n",
        "\n",
        "### Recomendaciones por caso de uso\n",
        "\n",
        "**Para documentos técnicos largos:**\n",
        "- Chunks medianos (500-800 caracteres)\n",
        "- Metadata por sección y dificultad\n",
        "- Ensemble retriever con reordenamiento\n",
        "\n",
        "**Para preguntas específicas:**\n",
        "- Chunks pequeños (200-300 caracteres)\n",
        "- Filtros de metadata estrictos\n",
        "- K bajo (1-3) para mayor precisión\n",
        "\n",
        "**Para consultas complejas:**\n",
        "- Chunks grandes (1000+ caracteres)\n",
        "- Multi-query retriever\n",
        "- K alto (5-7) para mayor cobertura\n",
        "\n",
        "**Para documentos jerárquicos:**\n",
        "- Parent Document Retriever\n",
        "- Chunking por secciones\n",
        "- Metadata de ubicación\n",
        "\n",
        "### Recursos adicionales\n",
        "\n",
        "- [LangChain Documentation](https://python.langchain.com/)\n",
        "- [Pinecone Documentation](https://docs.pinecone.io/)\n",
        "- [RAG Best Practices](https://docs.llamaindex.ai/en/stable/optimizing/rag_optimization.html)\n",
        "- [Advanced RAG Techniques](https://arxiv.org/abs/2312.10997)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

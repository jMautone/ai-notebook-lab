{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1kGcqxh1Z6s"
      },
      "source": [
        "# Laboratorio 4: Agents y Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1d0520"
      },
      "source": [
        "## IntroducciÃ³n\n",
        "\n",
        "En este laboratorio prÃ¡ctico sobre Agentes y Tools explorarÃ¡s cÃ³mo construir agentes inteligentes utilizando las librerÃ­as LangChain y LangGraph.\n",
        "\n",
        "A travÃ©s de una serie de ejercicios prÃ¡cticos aprenderÃ¡s a:\n",
        "\n",
        "- Crear tu primer Agente ReAct.\n",
        "- Integrar mÃºltiples tools en un agente.\n",
        "- Construir un chatbot con LangGraph.\n",
        "- Agregar herramientas externas como Wikipedia y APIs de clima.\n",
        "- Incorporar una base de conocimiento utilizando un Ã­ndice de Pinecone.\n",
        "- Implementar condiciones personalizadas para la selecciÃ³n de tools.\n",
        "\n",
        "Este laboratorio estÃ¡ diseÃ±ado para ser un espacio de prÃ¡ctica donde podrÃ¡s experimentar y aplicar los conceptos de agentes y tools en un entorno interactivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DCVKnlNo-A3A"
      },
      "outputs": [],
      "source": [
        "%pip install -U \\\n",
        "  langchain==0.3.7 \\\n",
        "  langchain-core==0.3.18 \\\n",
        "  langchain-openai==0.2.3 \\\n",
        "  langchain-community==0.3.7 \\\n",
        "  langchain-pinecone==0.2.0 \\\n",
        "  langchain-huggingface==0.1.2 \\\n",
        "  langgraph==0.2.19 \\\n",
        "  sentence-transformers \\\n",
        "  pinecone-client \\\n",
        "  pydantic==2.9.2 \\\n",
        "  pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiKJBKNKGcU8"
      },
      "source": [
        "## Parte 1: Creando tu primer ReAct Agent\n",
        "\n",
        "Crea un agente ReAct que use un modelo de lenguaje (`ChatOpenAI`) y una tool para contar cuÃ¡ntas letras \"r\" hay en una palabra.\n",
        "\n",
        "El agente debe recibir una pregunta del usuario, invocar la herramienta y responder con el resultado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-nG0wDi1YNW"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from google.colab import userdata\n",
        "\n",
        "# âœ… Inicializar el modelo de lenguaje gpt-4o-mini usando ChatOpenAI\n",
        "model = ChatOpenAI(api_key=userdata.get('OPENAI_API_TOKEN'), model=\"gpt-4o-mini\")\n",
        "\n",
        "# âœ… Definir una tool que cuente las 'r' en una palabra\n",
        "@tool\n",
        "def count_r_in_word(word: str) -> int:\n",
        "    \"\"\"Count how many 'r' letters are in the given word.\"\"\"\n",
        "    return word.lower().count('r')\n",
        "\n",
        "# âœ… Crear el agente con create_react_agent\n",
        "app = create_react_agent(model=model, tools=[count_r_in_word])\n",
        "\n",
        "# Pregunta del usuario\n",
        "query = \"How many r's are in the word 'Terrarium'?\"\n",
        "\n",
        "# âœ… Invocar el agente y mostrar la respuesta final\n",
        "response = app.invoke({\"messages\": [(\"user\", query)]})\n",
        "print(response['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NsMrvxdBG6A"
      },
      "source": [
        "## Parte 2: Agregando mÃºltiples tools\n",
        "\n",
        "AmplÃ­a el agente para incluir una segunda herramienta que calcule el Ã¡rea de un rectÃ¡ngulo.\n",
        "\n",
        "Luego, haz una consulta que combine ambas herramientas en una sola interacciÃ³n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG9Jne9oEvNL"
      },
      "outputs": [],
      "source": [
        "# âœ… Define una tool que calcule el Ã¡rea de un rectÃ¡ngulo\n",
        "@tool\n",
        "def calculate_rectangle_area(length: float, width: float) -> float:\n",
        "    \"\"\"Calculate the area of a rectangle given its length and width.\"\"\"\n",
        "    return length * width\n",
        "\n",
        "# âœ… Crear el agente con ambas tools\n",
        "app = create_react_agent(model=model, tools=[count_r_in_word, calculate_rectangle_area])\n",
        "\n",
        "query = \"What is the area of a rectangle with length 5 and width 11? and how many r's are in 'Race'?\"\n",
        "\n",
        "# âœ… Invocar y mostrar la respuesta\n",
        "response = app.invoke({\"messages\": [(\"user\", query)]})\n",
        "print(response['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBcLoRnsGV8E"
      },
      "source": [
        "## Parte 3: Construyendo un chatbot con LangGraph\n",
        "\n",
        "Crea un `StateGraph` bÃ¡sico con un solo nodo chatbot que responda preguntas directamente usando el modelo LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98w8jdehGpZX"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# âœ… Inicializa el modelo LLM\n",
        "llm = ChatOpenAI(api_key=userdata.get('OPENAI_API_TOKEN'), model=\"gpt-4o-mini\")\n",
        "\n",
        "# âœ… Crea el grafo y define el nodo chatbot\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "def chatbot(state: MessagesState):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "# âœ… Agrega nodos y bordes al grafo\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# âœ… Invoca el grafo con una consulta\n",
        "response = graph.invoke({\"messages\": [(\"human\", \"Who is Ada Lovelace?\")]})\n",
        "print(response[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG8oQP8uJZZ7"
      },
      "source": [
        "## Parte 4: Agregando WikipediaTool y usando stream_tool_responses\n",
        "\n",
        "Integra la tool de Wikipedia al chatbot y usa la funciÃ³n `stream_tool_responses` para observar cÃ³mo el agente decide cuÃ¡ndo invocar la herramienta y cÃ³mo se entregan las respuestas en streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC2V5ryDK2IH"
      },
      "outputs": [],
      "source": [
        "%pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCLCEbcnIxKz"
      },
      "outputs": [],
      "source": [
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# âœ… Inicializa el wrapper de Wikipedia\n",
        "api_wrapper = WikipediaAPIWrapper()\n",
        "wikipedia_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "\n",
        "tools = [wikipedia_tool]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: MessagesState):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# âœ… Crea el grafo y define el nodo chatbot\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# âœ… Crea el nodo de herramientas y define las transiciones\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "def stream_tool_responses(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
        "        for item in event.values():\n",
        "            print(\"Agent:\", item[\"messages\"])\n",
        "\n",
        "query = \"Who is Alan Turing?\"\n",
        "stream_tool_responses(query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io8zD2FtJWpq"
      },
      "source": [
        "## Parte 5: Agregando una Tool de API externa\n",
        "\n",
        "Agrega una tool que obtenga el clima actual de una ciudad usando la API de Open-Meteo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mnc4bBSQJYiP"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# âœ… Completar la funciÃ³n para obtener el clima actual\n",
        "@tool\n",
        "def weather_tool(city: str) -> str:\n",
        "    \"\"\"\n",
        "    Retrieve current weather for a city using Open-Meteo.\n",
        "    \"\"\"\n",
        "    # âœ… Paso 1: GeocodificaciÃ³n para obtener latitud y longitud\n",
        "    geo_url = f\"https://geocoding-api.open-meteo.com/v1/search?name={city}\"\n",
        "    geo_resp = requests.get(geo_url).json()\n",
        "    if \"results\" not in geo_resp or len(geo_resp[\"results\"]) == 0:\n",
        "        return f\"Could not find location for {city}.\"\n",
        "\n",
        "    lat = geo_resp[\"results\"][0][\"latitude\"]\n",
        "    lon = geo_resp[\"results\"][0][\"longitude\"]\n",
        "\n",
        "    # âœ… Paso 2: Obtener datos del clima actual\n",
        "    weather_url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
        "    weather_resp = requests.get(weather_url).json()\n",
        "\n",
        "    if \"current_weather\" not in weather_resp:\n",
        "        return f\"Could not retrieve weather data for {city}.\"\n",
        "\n",
        "    temp = weather_resp[\"current_weather\"][\"temperature\"]\n",
        "    wind = weather_resp[\"current_weather\"][\"windspeed\"]\n",
        "    return f\"The current temperature in {city} is {temp}Â°C with a wind speed of {wind} km/h.\"\n",
        "\n",
        "# âœ… Agrega la nueva tool al grafo\n",
        "tools = [wikipedia_tool, weather_tool]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# âœ… Volver a crear y compilar el grafo\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "graph_builder.add_node(\"chatbot\", lambda state: {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]})\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# ðŸ§ª Espacio para hacer pruebas con consultas\n",
        "query = \"What's the weather like in Montevideo and who was Nikola Tesla?\"\n",
        "response = graph.invoke({\"messages\": [(\"user\", query)]})\n",
        "print(response[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0i0ZQEWPSaD"
      },
      "source": [
        "## Parte 6: Agregando una base de conocimiento\n",
        "\n",
        "Agrega una tool que consulte un Ã­ndice de Pinecone con embeddings de un documento PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKVIx5LhYSci"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# --- Configurar credenciales para HuggingFace y Pinecone ---\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = userdata.get(\"HUGGING_API_KEY\")\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "# --- Cargar un archivo PDF desde el entorno de Colab ---\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "print(f\"ðŸ“„ Archivo cargado: {pdf_path}\")\n",
        "\n",
        "# --- Leer el contenido del PDF y dividirlo en fragmentos manejables ---\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"âœ… Documento dividido en {len(chunks)} fragmentos\")\n",
        "\n",
        "# --- Crear embeddings y conectarse a Pinecone ---\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "index_name = \"docs-index\"\n",
        "\n",
        "# --- Crear el Ã­ndice si aÃºn no existe ---\n",
        "if index_name not in [idx[\"name\"] for idx in pc.list_indexes()]:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,  # dimensiÃ³n del modelo de embeddings usado\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "    print(f\"âœ… Ãndice '{index_name}' creado correctamente.\")\n",
        "else:\n",
        "    print(f\"â„¹ï¸ El Ã­ndice '{index_name}' ya existe.\")\n",
        "\n",
        "# --- Generar y subir los embeddings del PDF al Ã­ndice Pinecone ---\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "vector_store = PineconeVectorStore.from_documents(chunks, embeddings, index_name=index_name)\n",
        "print(\"âœ… Embeddings generados y cargados en Pinecone correctamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clN4jLC_LfAo"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# ConexiÃ³n y uso de la base de conocimiento (Pinecone)\n",
        "# ==========================================\n",
        "\n",
        "# Se conecta a un Ã­ndice existente en Pinecone que ya contiene los embeddings del PDF\n",
        "vector_store = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)\n",
        "print(f\"ðŸ”— Conectado al Ã­ndice existente '{index_name}' en Pinecone.\")\n",
        "\n",
        "\n",
        "# --- Tool de consulta: accede al Ã­ndice para responder preguntas del PDF ---\n",
        "@tool\n",
        "def db_knowledge(query: str) -> str:\n",
        "    \"\"\"Busca fragmentos relevantes en el Ã­ndice Pinecone y devuelve su contenido.\"\"\"\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    if not docs:\n",
        "        return \"No relevant knowledge found.\"\n",
        "    return \"\\n\\n---\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "\n",
        "# --- ConstrucciÃ³n del grafo LangGraph ---\n",
        "# El grafo define cÃ³mo el agente decide entre responder o usar la tool.\n",
        "tools = [db_knowledge]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "def chatbot(state: MessagesState):\n",
        "    # InstrucciÃ³n para que el modelo use la tool si la pregunta estÃ¡ relacionada con el PDF\n",
        "    system_prompt = (\n",
        "        \"You have access to a Pinecone vector store built from a PDF. \"\n",
        "        \"If the question can be answered using that PDF, call the 'db_knowledge' tool first.\"\n",
        "    )\n",
        "    messages = [(\"system\", system_prompt)] + state[\"messages\"]\n",
        "    return {\"messages\": [llm_with_tools.invoke(messages)]}\n",
        "\n",
        "# Grafo con dos nodos: el chatbot y las tools\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Define el flujo: si el modelo llama una tool â†’ ir a 'tools'; si no â†’ terminar (END)\n",
        "graph_builder.add_conditional_edges(\"chatbot\", tools_condition, {\"tools\": \"tools\", END: END})\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "graph = graph_builder.compile()\n",
        "print(\"âœ… Grafo compilado correctamente (usando Ã­ndice existente).\")\n",
        "\n",
        "\n",
        "# --- Ejemplo de consulta ---\n",
        "# Se envÃ­a una pregunta al agente, que decidirÃ¡ si usar la base de conocimiento.\n",
        "query = \"Summarize the main ideas discussed in the uploaded PDF document.\"\n",
        "response = graph.invoke({\"messages\": [(\"user\", query)]})\n",
        "\n",
        "print(\"\\nðŸ¤– Respuesta del agente:\\n\")\n",
        "print(response[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN2CcL3fDCc_"
      },
      "source": [
        "## Parte 7: Condiciones personalizadas para decisiÃ³n de uso de tools\n",
        "\n",
        "Crea un agente que decida entre las diferentes tools segÃºn el contenido del mensaje del usuario:\n",
        "\n",
        "- Si el mensaje menciona \"clima\" â†’ invocar `weather_tool`.\n",
        "- Si menciona \"wiki\" â†’ invocar `wikipedia_tool`.\n",
        "- Si menciona \"documento\" â†’ invocar `db_knowledge`.\n",
        "- Para cualquier otro mensaje â†’ invocar al LLM y terminar el flujo sin llamar a ninguna tool.\n",
        "\n",
        "**Nota:** Tener en cuenta que cada tool debe ser creada en un nodo independiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0GdfTfFeDtWX"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from IPython.display import Image, display\n",
        "import re\n",
        "\n",
        "def call_wikipedia_tool(query: str) -> str:\n",
        "    \"\"\"Llama a la tool de Wikipedia que ya creaste.\"\"\"\n",
        "    # WikipediaQueryRun expone `.run(texto)`\n",
        "    return wikipedia_tool.run(query)\n",
        "\n",
        "\n",
        "def extract_city_from_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    HeurÃ­stica simple: agarra la palabra despuÃ©s de 'en' o 'in'.\n",
        "    Si no encuentra nada, devuelve todo el texto (fallback).\n",
        "    \"\"\"\n",
        "    # Intentamos capturar \"en Montevideo\", \"in Montevideo\", etc.\n",
        "    m = re.search(r\"(?:en|in)\\s+([A-Za-zÃÃ‰ÃÃ“ÃšÃ¡Ã©Ã­Ã³ÃºÃ±Ã‘]+)\", text)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "    return text\n",
        "\n",
        "\n",
        "def call_weather_tool_from_text(text: str) -> str:\n",
        "    \"\"\"Extrae ciudad y llama a @tool weather_tool(city: str).\"\"\"\n",
        "    city = extract_city_from_text(text)\n",
        "    # Por seguridad usamos invoke con dict de argumentos\n",
        "    result = weather_tool.invoke({\"city\": city})\n",
        "    return result\n",
        "\n",
        "\n",
        "def call_db_knowledge(query: str) -> str:\n",
        "    \"\"\"Llama a la tool db_knowledge(query: str) sobre Pinecone.\"\"\"\n",
        "    result = db_knowledge.invoke({\"query\": query})\n",
        "    return result\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Nodos del grafo\n",
        "# -----------------------\n",
        "\n",
        "def weather_tool_node(state: MessagesState):\n",
        "    user_text = state[\"messages\"][-1].content\n",
        "\n",
        "    tool_output = call_weather_tool_from_text(user_text)\n",
        "\n",
        "    tool_msg = AIMessage(\n",
        "        content=f\"[weather_tool]\\n{tool_output}\",\n",
        "        name=\"weather_tool\"\n",
        "    )\n",
        "    return {\"messages\": state[\"messages\"] + [tool_msg]}\n",
        "\n",
        "\n",
        "def wikipedia_tool_node(state: MessagesState):\n",
        "    user_text = state[\"messages\"][-1].content\n",
        "\n",
        "    tool_output = call_wikipedia_tool(user_text)\n",
        "\n",
        "    tool_msg = AIMessage(\n",
        "        content=f\"[wikipedia_tool]\\n{tool_output}\",\n",
        "        name=\"wikipedia_tool\"\n",
        "    )\n",
        "    return {\"messages\": state[\"messages\"] + [tool_msg]}\n",
        "\n",
        "\n",
        "def db_knowledge_node(state: MessagesState):\n",
        "    user_text = state[\"messages\"][-1].content\n",
        "\n",
        "    tool_output = call_db_knowledge(user_text)\n",
        "\n",
        "    tool_msg = AIMessage(\n",
        "        content=f\"[db_knowledge]\\n{tool_output}\",\n",
        "        name=\"db_knowledge\"\n",
        "    )\n",
        "    return {\"messages\": state[\"messages\"] + [tool_msg]}\n",
        "\n",
        "\n",
        "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
        "\n",
        "def chatbot_node(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Nodo final que llama al LLM.\n",
        "    - Identifica quÃ© tool se usÃ³ (si alguna).\n",
        "    - Instruye al modelo para que use su salida y no diga que no tiene acceso.\n",
        "    - La respuesta del modelo SIEMPRE empieza con 'TOOL USADA: ...'.\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # 1) Detectar la Ãºltima tool usada (si la hay)\n",
        "    tool_used = \"ninguna\"\n",
        "    for msg in reversed(messages):\n",
        "        if isinstance(msg, AIMessage) and msg.name is not None:\n",
        "            tool_used = msg.name\n",
        "            break\n",
        "\n",
        "    # 2) System prompt fuerte para el modelo\n",
        "    system_text = f\"\"\"\n",
        "Eres un asistente que responde al usuario basÃ¡ndote en el historial de mensajes.\n",
        "\n",
        "- Si ves mensajes de herramientas (AIMessage con name 'weather_tool', 'wikipedia_tool' o 'db_knowledge'),\n",
        "  DEBES usar su contenido como fuente principal de tu respuesta.\n",
        "- Si la herramienta 'weather_tool' fue utilizada, NUNCA digas que no tienes acceso a informaciÃ³n en tiempo real;\n",
        "  simplemente confÃ­a en el resultado que la herramienta entregÃ³.\n",
        "- Si la herramienta 'wikipedia_tool' fue utilizada, responde usando el texto de Wikipedia como base.\n",
        "- Si la herramienta 'db_knowledge' fue utilizada, responde usando como base el contenido devuelto del Ã­ndice Pinecone.\n",
        "- Si no se usÃ³ ninguna herramienta, responde normalmente solo con tu conocimiento.\n",
        "\n",
        "IMPORTANTE:\n",
        "- Tu respuesta SIEMPRE debe comenzar con una lÃ­nea exactamente asÃ­:\n",
        "  TOOL USADA: {tool_used}\n",
        "  (si no se usÃ³ herramienta, debes poner 'TOOL USADA: ninguna')\n",
        "- DespuÃ©s de esa lÃ­nea, escribe la explicaciÃ³n o respuesta al usuario.\n",
        "\"\"\"\n",
        "\n",
        "    full_messages = [SystemMessage(content=system_text)] + messages\n",
        "\n",
        "    # 3) Llamar al modelo\n",
        "    ai_resp = llm.invoke(full_messages)\n",
        "\n",
        "    # No ponemos name al chatbot, para poder distinguir tool vs. respuesta final\n",
        "    return {\"messages\": state[\"messages\"] + [ai_resp]}\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Router (condiciones)\n",
        "# -----------------------\n",
        "\n",
        "def tool_selector(state: MessagesState):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "\n",
        "    if not isinstance(last_message, HumanMessage):\n",
        "        return \"chatbot\"\n",
        "\n",
        "    content = last_message.content.lower()\n",
        "\n",
        "    if \"clima\" in content:\n",
        "        return \"weather_tool_node\"\n",
        "    if \"wiki\" in content:\n",
        "        return \"wikipedia_tool_node\"\n",
        "    if \"documento\" in content:\n",
        "        return \"db_knowledge_node\"\n",
        "\n",
        "    # Cualquier otro mensaje â†’ directo al chatbot\n",
        "    return \"chatbot\"\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# ConstrucciÃ³n del grafo\n",
        "# -----------------------\n",
        "\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# Registrar nodos\n",
        "graph_builder.add_node(\"weather_tool_node\", weather_tool_node)\n",
        "graph_builder.add_node(\"wikipedia_tool_node\", wikipedia_tool_node)\n",
        "graph_builder.add_node(\"db_knowledge_node\", db_knowledge_node)\n",
        "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
        "\n",
        "# Desde START elegimos a quÃ© nodo ir\n",
        "graph_builder.add_conditional_edges(\n",
        "    START,\n",
        "    tool_selector,\n",
        "    [\"weather_tool_node\", \"wikipedia_tool_node\", \"db_knowledge_node\", \"chatbot\"],\n",
        ")\n",
        "\n",
        "# Si pasamos por una tool, luego vamos al chatbot\n",
        "graph_builder.add_edge(\"weather_tool_node\", \"chatbot\")\n",
        "graph_builder.add_edge(\"wikipedia_tool_node\", \"chatbot\")\n",
        "graph_builder.add_edge(\"db_knowledge_node\", \"chatbot\")\n",
        "\n",
        "# El chatbot siempre cierra el flujo\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaYwawVi9LSo"
      },
      "source": [
        "### Interactuar con el modelo\n",
        "\n",
        "Ejecuta las siguientes pruebas para validar el funcionamiento del agente con diferentes tipos de consultas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dekanm471WhP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "print(\"=== CLIMA ===\")\n",
        "res_clima = graph.invoke({\"messages\": [HumanMessage(content=\"Â¿CÃ³mo estÃ¡ el clima hoy en Montevideo?\")]})\n",
        "for m in res_clima[\"messages\"]:\n",
        "    print(type(m).__name__, \"->\", getattr(m, \"name\", None), \"\\n\", m.content, \"\\n\")\n",
        "\n",
        "print(\"=== WIKI ===\")\n",
        "res_wiki = graph.invoke({\"messages\": [HumanMessage(content=\"BuscÃ¡ en la wiki informaciÃ³n sobre la inteligencia artificial.\")]})\n",
        "for m in res_wiki[\"messages\"]:\n",
        "    print(type(m).__name__, \"->\", getattr(m, \"name\", None), \"\\n\", m.content, \"\\n\")\n",
        "\n",
        "print(\"=== DOCUMENTO ===\")\n",
        "res_doc = graph.invoke({\"messages\": [HumanMessage(content=\"Explicame el contenido del documento que cargamos.\")]})\n",
        "for m in res_doc[\"messages\"]:\n",
        "    print(type(m).__name__, \"->\", getattr(m, \"name\", None), \"\\n\", m.content, \"\\n\")\n",
        "\n",
        "print(\"=== CHAT DIRECTO ===\")\n",
        "res_chat = graph.invoke({\"messages\": [HumanMessage(content=\"Contame un chiste corto.\")]})\n",
        "for m in res_chat[\"messages\"]:\n",
        "    print(type(m).__name__, \"->\", getattr(m, \"name\", None), \"\\n\", m.content, \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

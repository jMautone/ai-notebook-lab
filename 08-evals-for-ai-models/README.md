# ğŸ§ª Lab 8: EvaluaciÃ³n de Modelos de IA - SoluciÃ³n Completa

Este directorio contiene la soluciÃ³n completa para el Lab 8 del curso de IA con Ã©nfasis en evaluaciÃ³n de modelos.

## ğŸ“‹ Contenidos

- **`solution_lab8.py`**: SoluciÃ³n completa con los 3 ejercicios
- **`.env`**: Archivo de configuraciÃ³n (necesita tu API key de OpenAI)
- **`requirements.txt`**: Dependencias del proyecto

## ğŸš€ Instrucciones de InstalaciÃ³n y EjecuciÃ³n

### 1ï¸âƒ£ Configurar Variables de Entorno

Edita el archivo `.env` y reemplaza la API key:

```bash
# Archivo: .env
OPENAI_API_KEY=sk-proj-tu-clave-real-aqui
```

ObtÃ©n tu clave en: https://platform.openai.com/api-keys

### 2ï¸âƒ£ Instalar Dependencias

```powershell
cd 08-evals-for-ai-models
pip install -r requirements.txt
```

### 3ï¸âƒ£ Ejecutar la SoluciÃ³n

```powershell
python solution_lab8.py
```

## ğŸ“Š Estructura de la SoluciÃ³n

### âœ… Ejercicio 1: Dataset Personalizado

Crea un dataset con **5 pares de evaluaciÃ³n** (pregunta, contexto, respuesta de referencia):

**Temas incluidos:**
- Historia: RevoluciÃ³n Industrial
- BiologÃ­a: FotosÃ­ntesis
- Ciencia: Cambio ClimÃ¡tico
- TecnologÃ­a: Ada Lovelace
- Salud: Beneficios del Ejercicio

**Requisitos cumplidos:**
- âœ… MÃ­nimo 5 pares
- âœ… Contextos detallados y suficientes
- âœ… Preguntas claras y bien formuladas
- âœ… Respuestas de referencia precisas
- âœ… Coherencia pregunta-contexto-respuesta

### âœ… Ejercicio 2: EvaluaciÃ³n con Faithfulness (RAGAS)

EvalÃºa la **fidelidad de las respuestas al contexto** usando OpenAI como juez:

```
Faithfulness = Â¿QuÃ© tan fiel es la respuesta al contexto sin alucinar?

Score: 0.0 (completamente alucinada) a 1.0 (100% fiel)
```

**Pasos:**
1. Genera respuestas con LLM basadas Ãºnicamente en el contexto
2. EvalÃºa cada respuesta usando OpenAI como juez
3. Produce tabla de resultados con scores

**Resultados:**
- Tabla con scores por pregunta
- EstadÃ­sticas (media, min, max, desv. estÃ¡ndar)
- IdentificaciÃ³n de alucinaciones

### âœ… Ejercicio 3: MÃ©trica Personalizada - Completitud

Implementa una **mÃ©trica personalizada** que evalÃºa completitud:

```
Completitud = Â¿La respuesta cubre todos los aspectos preguntados?

ComparaciÃ³n: Respuesta generada vs Respuesta de referencia (ground truth)
Score: 0.0 (no cubre nada) a 1.0 (cubre todos los puntos)
```

**Detalles:**
- Tipo: Completitud de Respuesta
- Compara contra respuesta de referencia
- EvalÃºa cobertura de puntos clave
- IdentificaciÃ³n de puntos cubiertos y faltantes

## ğŸ“ Archivos Generados

Cuando ejecutas `solution_lab8.py`, se generan automÃ¡ticamente:

| Archivo | DescripciÃ³n |
|---------|-------------|
| `faithfulness_results.csv` | Resultados detallados de Faithfulness |
| `completeness_results.csv` | Resultados detallados de Completitud |
| `evaluation_results.png` | GrÃ¡fico comparativo de scores |
| `evaluation_report.txt` | Reporte completo en texto |

## ğŸ“Š Ejemplo de Salida

```
================================================================================
ğŸ§ª LAB 8: EVALUACIÃ“N DE MODELOS DE IA
================================================================================

ğŸ“Œ EJERCICIO 1: Crear Dataset Personalizado
âœ… Dataset creado con 5 pares pregunta-respuesta

ğŸ“Š EJERCICIO 2: Evaluar Faithfulness de RAGAS
âœ… Generando respuestas con LLM...
âœ… Evaluando Faithfulness...

ğŸ“ˆ RESULTADOS FAITHFULNESS:
   Score promedio: 0.85
   MÃ¡ximo: 0.95
   MÃ­nimo: 0.72

ğŸ“‹ EJERCICIO 3: MÃ©trica Personalizada - Completitud
âœ… Evaluando Completitud de Respuestas...

ğŸ“ˆ RESULTADOS COMPLETITUD:
   Score promedio: 0.88
   MÃ¡ximo: 0.98
   MÃ­nimo: 0.75

âœ¨ Â¡LABORATORIO COMPLETADO EXITOSAMENTE! âœ¨
```

## ğŸ”§ Troubleshooting

### Error: "OPENAI_API_KEY no configurada"

SoluciÃ³n:
1. Edita `.env` con tu API key real
2. AsegÃºrate de no tener espacios en blanco: `OPENAI_API_KEY=sk-proj-xxx`

### Error: "No module named 'openai'"

SoluciÃ³n:
```powershell
pip install -r requirements.txt
```

### Error: "API rate limit"

Si obtienes errores de rate limit, reduce el nÃºmero de preguntas en `create_custom_dataset()` o agrega delays entre llamadas.

## ğŸ“š Recursos Recomendados

- **RAGAS Docs**: https://docs.ragas.io/
- **OpenAI API**: https://platform.openai.com/docs/api-reference
- **Pandas**: https://pandas.pydata.org/
- **Matplotlib**: https://matplotlib.org/

## ğŸ¯ Criterios de EvaluaciÃ³n

### Dataset (Ejercicio 1)
- âœ… 5+ pares de (pregunta, contexto, respuesta)
- âœ… Contextos ricos y suficientemente informativos
- âœ… Respuestas precisas y completas
- âœ… Coherencia entre componentes

### Faithfulness (Ejercicio 2)
- âœ… MÃ©trica correctamente implementada
- âœ… AnÃ¡lisis profundo de resultados
- âœ… Visualizaciones claras
- âœ… Tabla de resultados

### MÃ©trica Personalizada (Ejercicio 3)
- âœ… Bien documentada y explicada
- âœ… Validada con casos de prueba
- âœ… CÃ³digo limpio y legible
- âœ… IntegraciÃ³n coherente

## ğŸ“ Notas

- El script usa `gpt-4o-mini` para evaluaciones rÃ¡pidas y econÃ³micas
- Todas las evaluaciones se basan en contenido especÃ­fico del contexto proporcionado
- Los resultados se guardan en CSV para anÃ¡lisis posterior
- Los grÃ¡ficos se generan automÃ¡ticamente

## âœ¨ Tips para Mejorar Resultados

1. **Ajusta la temperatura** en las llamadas LLM segÃºn necesites (actualmente: 0.7 para generaciÃ³n, 0.3 para evaluaciÃ³n)
2. **Enriquece los contextos** con mÃ¡s detalles para mejorar faithfulness
3. **Prueba diferentes modelos** (gpt-4, gpt-3.5-turbo) segÃºn tu presupuesto
4. **Visualiza los resultados** para identificar patrones

---

**Autor**: SoluciÃ³n Automatizada Lab 8
**Fecha**: 2025-11-17

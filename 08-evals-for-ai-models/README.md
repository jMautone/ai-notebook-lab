# üß™ Lab 8: Sistema de Evaluaci√≥n de Modelos de IA con RAGAS

Sistema completo de evaluaci√≥n de respuestas generadas por IA que implementa los 3 ejercicios del Lab 8.

## ‚úÖ Soluci√≥n Implementada

- **Ejercicio 1**: Dataset con 5 pares pregunta-contexto-respuesta
- **Ejercicio 2**: M√©trica Faithfulness de RAGAS + visualizaciones
- **Ejercicio 3**: 3 m√©tricas personalizadas (Formalidad, Completitud, Claridad)

---

## üöÄ Inicio R√°pido

### 1. Instalar Dependencias

```bash
cd 08-evals-for-ai-models/ragas-evals
pip install -r requirements.txt
```

### 2. Configurar API Key

**Opci√≥n A: Archivo .env (Recomendado)**
```bash
cp .env.example .env
# Editar .env y agregar: OPENAI_API_KEY=sk-proj-tu-clave-aqui
```

**Opci√≥n B: Variable de entorno**
```powershell
$env:OPENAI_API_KEY = "sk-proj-tu-clave-aqui"
```

Obtener clave: https://platform.openai.com/api-keys

### 3. Ejecutar

```bash
python evals.py
```

**Salida**: Resultados en consola + 3 gr√°ficos PNG en `experiments/` + CSV con scores

---

## üìä Resultados

**Archivos generados:**
```
experiments/
‚îú‚îÄ‚îÄ metricas_comparacion.png  # Barras por pregunta
‚îú‚îÄ‚îÄ metricas_promedios.png    # Promedios por m√©trica
‚îú‚îÄ‚îÄ metricas_heatmap.png      # Mapa de calor
‚îî‚îÄ‚îÄ *.csv                     # Scores tabulados
```

**M√©tricas calculadas:**
- **Faithfulness** (RAGAS): Fidelidad al contexto
- **Formalidad**: Tono profesional
- **Completitud**: Cobertura de conceptos
- **Claridad**: Legibilidad y concisi√≥n

**Interpretaci√≥n de scores:**
- ‚â• 0.8: ‚úÖ Excelente
- 0.6-0.8: ‚ö†Ô∏è Bueno
- < 0.6: ‚ùå Mejorar

---

## üìÇ Estructura del Proyecto

```
ragas-evals/
‚îú‚îÄ‚îÄ evals.py              # üéØ Script principal (EJECUTAR ESTE)
‚îú‚îÄ‚îÄ custom_metrics.py     # 3 m√©tricas personalizadas (Ejercicio 3)
‚îú‚îÄ‚îÄ rag.py               # Sistema RAG + contextos
‚îú‚îÄ‚îÄ requirements.txt     # Dependencias
‚îú‚îÄ‚îÄ .env                 # Tu API key (crear)
‚îÇ
‚îú‚îÄ‚îÄ experiments/         # üìä Resultados (PNG + CSV)
‚îú‚îÄ‚îÄ logs/               # üìã Logs de ejecuci√≥n
‚îî‚îÄ‚îÄ datasets/           # üíæ Dataset guardado
```

---

## üèóÔ∏è Arquitectura

**Flujo de ejecuci√≥n:**
```
evals.py
  ‚îú‚îÄ> load_dataset()          ‚Üí 5 preguntas (Ejercicio 1)
  ‚îú‚îÄ> run_experiment()        ‚Üí Por cada pregunta:
  ‚îÇ    ‚îú‚îÄ> rag.query()        ‚Üí Genera respuesta con GPT-4o-mini
  ‚îÇ    ‚îú‚îÄ> Faithfulness       ‚Üí Score (Ejercicio 2)
  ‚îÇ    ‚îú‚îÄ> FormalidadMetric   ‚Üí Score (Ejercicio 3A)
  ‚îÇ    ‚îú‚îÄ> CompletitudMetric  ‚Üí Score (Ejercicio 3B)
  ‚îÇ    ‚îî‚îÄ> ClaridadMetric     ‚Üí Score (Ejercicio 3C)
  ‚îî‚îÄ> main()                  ‚Üí Visualiza + exporta
```

**Componentes:**

- **`evals.py`** - Script Principal
  - **Prop√≥sito**: Orquestador del sistema de evaluaci√≥n completo
  - **Funciones**: Define dataset (Ejercicio 1), ejecuta experimento RAGAS, calcula 4 m√©tricas por pregunta, genera 3 visualizaciones profesionales, exporta a CSV/PNG
  - **Flujo**: `load_dataset()` ‚Üí `run_experiment()` (llama RAG + m√©tricas) ‚Üí `main()` (visualiza + guarda)

- **`custom_metrics.py`** - M√©tricas Personalizadas (Ejercicio 3)
  - **Prop√≥sito**: 3 m√©tricas personalizadas que heredan de `DiscreteMetric`
  - **FormalidadMetric** (3A): Eval√∫a tono profesional, detecta emojis, coloquialismos, contracciones, exclamaciones excesivas
  - **CompletitudMetric** (3B): Eval√∫a cobertura de conceptos, verifica preguntas m√∫ltiples, longitud, desarrollo de ideas, compara con referencia
  - **ClaridadMetric** (3C): Eval√∫a legibilidad, analiza diversidad l√©xica, longitud de oraciones, complejidad, repeticiones, uso de conectores
  - **Arquitectura**: Todas retornan score 0.0-1.0 usando an√°lisis determin√≠stico (regex, conteos - sin LLMs)

- **`rag.py`** - Sistema RAG
  - **Prop√≥sito**: Sistema Retrieval-Augmented Generation que genera las respuestas a evaluar
  - **DOCUMENTS**: 5 documentos con contexto (Revoluci√≥n Industrial, fotos√≠ntesis, cambio clim√°tico, Ada Lovelace, ejercicio)
  - **SimpleKeywordRetriever**: Recupera documentos por coincidencia de palabras clave
  - **ExampleRAG**: Pipeline completo (`retrieve()` ‚Üí `generate()` con GPT-4o-mini)
  - **Logging**: Guarda trazas JSON en `logs/` con timestamps

---

## üîß Soluci√≥n de Problemas

**Error: API key no encontrada**
```bash
# Verificar .env
cat .env

# O usar variable de entorno
$env:OPENAI_API_KEY = "sk-proj-..."
python evals.py
```

**Error: M√≥dulo no encontrado**
```bash
pip install -r requirements.txt
```

**Error: multiprocess en Python 3.12**
- Ya solucionado en c√≥digo (parche de compatibilidad)

---

## üìà Pr√≥ximos Pasos

1. **Expandir dataset**: 10-20 preguntas
2. **Optimizar RAG**: Embeddings para retrieval sem√°ntico
3. **M√°s m√©tricas**: Answer Relevancy, Context Precision
4. **Comparar modelos**: GPT-4, Claude, Llama

---

## üìö Referencias

- RAGAS: https://docs.ragas.io/
- OpenAI API: https://platform.openai.com/docs/
- RAG Pattern: https://research.ibm.com/blog/retrieval-augmented-generation-rag



# ğŸ§ª Lab 8: EvaluaciÃ³n de Modelos de IA

## ğŸ“Œ DescripciÃ³n

En este laboratorio aprenderÃ¡s a evaluar la calidad de las respuestas generadas por modelos de IA utilizando mÃ©tricas estÃ¡ndar y personalizadas. TrabajarÃ¡s con el framework **RAGAS** para medir la confiabilidad y precisiÃ³n de tus sistemas de generaciÃ³n de texto.

---

## ğŸ“‹ Ejercicio 1: Crear un Dataset Propio con Contexto

**Objetivo**: Construir un dataset de evaluaciÃ³n con pares pregunta-respuesta de referencia bien fundamentados.

### Requisitos:

- **MÃ­nimo 5 pares** de (pregunta, contexto, respuesta de referencia)
- **Temas sugeridos**: Historia, BiologÃ­a, Cultura General, GeografÃ­a, TecnologÃ­a, Ciencia
- **Contexto detallado**: InformaciÃ³n suficiente y precisa para que un modelo pueda responder correctamente
- **Respuestas de referencia**: Respuestas correctas y bien documentadas que servirÃ¡n como ground truth

### Formato esperado:

```python
dataset = {
    "questions": ["Â¿Pregunta 1?", "Â¿Pregunta 2?", ...],
    "contexts": [["Contexto para pregunta 1"], ["Contexto para pregunta 2"], ...],
    "answers": ["Respuesta de referencia 1", "Respuesta de referencia 2", ...]
}
```

### Criterios de calidad:
- âœ… Contextos suficientemente informativos para responder
- âœ… Preguntas claras y bien formuladas
- âœ… Respuestas de referencia precisas y completas
- âœ… Coherencia entre pregunta, contexto y respuesta

---

## ğŸ“Š Ejercicio 2: Evaluar con la MÃ©trica Faithfulness de RAGAS

**Objetivo**: Medir quÃ© tan fieles son las respuestas generadas al contexto proporcionado.

### Â¿QuÃ© es Faithfulness?

**Faithfulness** evalÃºa si la respuesta generada se basa Ãºnicamente en la informaciÃ³n del contexto proporcionado, sin agregar hechos no verificables o alucinar informaciÃ³n.

### Pasos a seguir:

1. **Generar respuestas** con un LLM (usando el mismo contexto de tu dataset)
2. **Instalar y configurar RAGAS**:
   ```bash
   pip install ragas
   ```
3. **Calcular la mÃ©trica Faithfulness** para cada par (contexto, respuesta generada)
4. **Analizar resultados**:
   - Score de Faithfulness por respuesta
   - Promedio general del dataset
   - Identificar respuestas con baja fidelidad (alucinaciones)

### Entregables:
- ğŸ“Š Tabla comparativa con scores de Faithfulness
- ğŸ“ˆ VisualizaciÃ³n de resultados
- ğŸ“ AnÃ¡lisis de quÃ© respuestas fallaron y por quÃ©

---

## ğŸ¯ Ejercicio 3: Crear una MÃ©trica Personalizada

**Objetivo**: Desarrollar una mÃ©trica customizada para evaluar aspecto especÃ­ficos de la calidad de respuestas.

### Opciones de mÃ©tricas sugeridas:

#### OpciÃ³n A: Formalidad del Tono
- EvalÃºa si la respuesta mantiene un **tono formal** y profesional
- No debe contener lenguaje coloquial, emojis o jerga informal
- Score: 0-1 basado en anÃ¡lisis de vocabulario y estructura

#### OpciÃ³n B: Completitud de Respuesta
- EvalÃºa si la respuesta cubre **todos los aspectos** preguntados
- Analiza si responde todas las sub-preguntas implÃ­citas
- Score: 0-1 basado en cobertura de informaciÃ³n

#### OpciÃ³n C: Claridad y ConcisiÃ³n
- Mide si la respuesta es **clara, directa y sin redundancias**
- EvalÃºa complejidad de lectura y estructura gramatical
- Score: 0-1 basado en mÃ©tricas de readability

### Pasos de implementaciÃ³n:

1. **Definir criterios claros** para tu mÃ©trica
2. **Crear funciÃ³n evaluadora** que:
   - Reciba como entrada: pregunta, contexto y respuesta generada
   - Retorne un score numÃ©rico entre 0 y 1
   - Incluya lÃ³gica de evaluaciÃ³n automÃ¡tica (puede usar LLM si es necesario)
3. **Integrar con RAGAS** (si es posible) o implementar evaluaciÃ³n standalone
4. **Validar resultados** con al menos 2-3 respuestas manualmente

### Entregables:
- ğŸ’» CÃ³digo limpio y bien documentado de la mÃ©trica
- ğŸ“Š Tabla de resultados para todas las respuestas del dataset
- ğŸ“‹ DocumentaciÃ³n de la lÃ³gica de evaluaciÃ³n
- ğŸ” Casos de ejemplo donde la mÃ©trica funciona correctamente

---

## ğŸ¯ Criterios de EvaluaciÃ³n

| Criterio | Excelente | Bueno | Satisfactorio |
|----------|-----------|-------|---------------|
| **Dataset** | 5+ pares, contextos ricos, respuestas precisas | 5 pares, contextos suficientes | 5 pares bÃ¡sicos |
| **Faithfulness** | AnÃ¡lisis profundo, visualizaciones claras | CÃ¡lculos correctos, tabla de resultados | MÃ©trica aplicada con mÃ­nimo anÃ¡lisis |
| **MÃ©trica Custom** | Bien documentada, validada, integrada | Funciona correctamente, cÃ³digo limpio | Implementada pero con limitaciones |

---

## ğŸ“š Recursos Recomendados

- **RAGAS Documentation**: https://docs.ragas.io/
- **RAGAS Metrics**: https://docs.ragas.io/en/latest/concepts/metrics/
- **LangChain Evaluation**: https://python.langchain.com/docs/modules/evaluation/
- **LLM as a Judge Pattern**: Usando LLMs para evaluar respuestas

---

## ğŸš€ Tips para el Ã‰xito

- âœ¨ Usa datasets de alta calidad; basura entrada = basura salida
- ğŸ” Prueba tus mÃ©tricas personalizadas con casos conocidos primero
- ğŸ“Š Visualiza los resultados para mejor comprensiÃ³n
- ğŸ’¬ Considera usar LLMs avanzados (GPT-4) para evaluaciones mÃ¡s precisas
- ğŸ”„ Itera sobre tus mÃ©tricas si los resultados no son coherentes
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBCSNaCdrBz-"
      },
      "source": [
        "# Laboratorio 1: Setup y uso básico de LLMs con LangChain + Prompt Engineering Avanzado\n",
        "\n",
        "Este laboratorio está pensado para completarse en ~2 horas. Trabajarás en Google Colab con recursos gratuitos, usando la Inference API de Hugging Face y un modelo instruct abierto.\n",
        "\n",
        "**Contenidos:**\n",
        "- Setup en Colab y configuración de Hugging Face Inference API\n",
        "- Uso básico de LLMs con LangChain (LCEL)\n",
        "- Parámetros de decodificación y control de estilo\n",
        "- Prompt engineering avanzado: zero-shot, few-shot, Chain of Thought, Role Prompting y salida estructurada (JSON)\n",
        "\n",
        "**Al finalizar, podrás:**\n",
        "- Conectarte a un LLM instruct vía Hugging Face Inference API desde LangChain\n",
        "- Construir cadenas simples con `prompt | llm | parser`\n",
        "- Diseñar prompts efectivos y controlar formato de salida (incluido JSON)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RTG7_ybrBz_"
      },
      "source": [
        "## Parte 0: Setup (Colab + librerías + token)\n",
        "\n",
        "Usarás versiones estables para minimizar fricción en Colab. Asegúrate de ejecutar esta sección antes de continuar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxoJSkLcrBz_"
      },
      "outputs": [],
      "source": [
        "# Instalar dependencias principales (versiones estables)\n",
        "!pip -q install -U \\\n",
        "  \"langchain==0.3.27\" \\\n",
        "  \"langchain-community==0.3.27\" \\\n",
        "  \"langchain-huggingface==0.3.1\" \\\n",
        "  \"transformers==4.55.2\" \\\n",
        "  \"huggingface_hub==0.34.4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmVbT5uRxn5R"
      },
      "outputs": [],
      "source": [
        "from importlib.metadata import version, PackageNotFoundError\n",
        "\n",
        "for dist in [\"langchain\", \"langchain-community\", \"langchain-huggingface\",\n",
        "             \"transformers\", \"huggingface_hub\"]:\n",
        "    try:\n",
        "        print(dist, \"=>\", version(dist))\n",
        "    except PackageNotFoundError:\n",
        "        print(dist, \"no instalado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9EJxPOCrB0A"
      },
      "outputs": [],
      "source": [
        "# Comprobar versión de Python y GPU/CPU\n",
        "import sys, subprocess, torch\n",
        "print(sys.version)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"PyTorch:\", torch.__version__)\n",
        "    print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"PyTorch no disponible o sin CUDA\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ug4tn-VrB0A"
      },
      "source": [
        "### Configuración del token de Hugging Face\n",
        "\n",
        "Para usar la Hugging Face Inference API necesitás un token personal (gratuito). En Colab se recomienda guardarlo en `userdata`:\n",
        "\n",
        "1. Crear el token en https://huggingface.co/settings/tokens\n",
        "2. En Colab: Abre el menú en la barra izquierda haciendo clic en la llave → \"Agregar nuevo secreto\", ponle `HF_TOKEN` y el token que generaste → Habilita el acceso al notebook\n",
        "3. Ejecutar la celda siguiente para leerlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMTYDzh9rB0A"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "assert HF_TOKEN is not None and len(HF_TOKEN) > 0, \"Configurar el secreto 'HF_TOKEN' en Colab.\"\n",
        "print(\"Token cargado OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQbt6t3FrB0A"
      },
      "source": [
        "## Parte 1: Uso básico de LLMs con LangChain\n",
        "\n",
        "Trabajarás con un modelo instruct accesible vía Inference API. Para minimizar fricción, usarás un modelo abierto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNzk__gZrB0B"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "\n",
        "hf_endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=MODEL_ID,\n",
        "    task=\"conversational\",\n",
        "    huggingfacehub_api_token=HF_TOKEN,\n",
        "    temperature=0.7,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "\n",
        "llm = ChatHuggingFace(llm=hf_endpoint)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente útil y conciso.\"),\n",
        "    (\"human\", \"Responde a la siguiente instrucción: {instruccion}\"),\n",
        "])\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "print(chain.invoke({\"instruccion\": \"Explica en 3 frases qué es un LLM y nombra 2 casos de uso.\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM0fs9ZLrB0B"
      },
      "source": [
        "### Ejercicio 1.1 (10 min)\n",
        "\n",
        "Realiza las siguientes tareas:\n",
        "- Probar 3 variaciones de `temperature` y observar el cambio en estilo\n",
        "- Cambiar el rol del `system` para forzar un estilo (por ejemplo: \"responde con viñetas y máximo 3 líneas\")\n",
        "- Pregunta sugerida: \"Resume la diferencia entre entrenamiento y fine-tuning en 3 puntos\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mcTYMM-srB0B"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "instruccion: str = \"Resume la diferencia entre entrenamiento y fine-tuning en 3 puntos.\"\n",
        "temperaturas: List[float] = [0.0, 0.7, 1.2]\n",
        "\n",
        "def ejecutar_variacion(temperature: float) -> str:\n",
        "    # TODO: crear endpoint conversacional con 'temperature' y devolver el texto\n",
        "    # Debe usar: MODEL_ID, HF_TOKEN, task=\"conversational\"\n",
        "    tmp_endpoint = HuggingFaceEndpoint(\n",
        "        repo_id=MODEL_ID,\n",
        "        task=\"conversational\",\n",
        "        huggingfacehub_api_token=HF_TOKEN,\n",
        "        temperature=temperature,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Eres un asistente útil y conciso.\"),\n",
        "        (\"human\", \"Responde a la siguiente instrucción: {instruccion}\"),\n",
        "    ])\n",
        "    chain = prompt | tmp_llm | StrOutputParser()\n",
        "    return chain.invoke({\"instruccion\": instruccion})\n",
        "\n",
        "\n",
        "for t in temperaturas:\n",
        "    print(f\"\\n==== temperature: {t} ====\")\n",
        "    print(ejecutar_variacion(t))\n",
        "\n",
        "def ejecutar_estilo(instruccion: str) -> str:\n",
        "    # TODO: crear prompt de estilo (system) + LLM base conversacional y devolver el texto\n",
        "    styled_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Responde con viñetas y máximo 3 líneas.\"),\n",
        "        (\"human\", \"Responde a la siguiente instrucción: {instruccion}\"),\n",
        "    ])\n",
        "    chain = styled_prompt | llm | StrOutputParser()\n",
        "    return chain.invoke({\"instruccion\": instruccion})\n",
        "\n",
        "\n",
        "print(\"\\n==== estilo forzado ====\")\n",
        "print(ejecutar_estilo(instruccion))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi_Cg8IMrB0C"
      },
      "source": [
        "### Parámetros de decodificación\n",
        "\n",
        "Ajustarás parámetros como `top_p`, `repetition_penalty` y `max_new_tokens` para observar su efecto en la generación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lCnXq_QrB0C"
      },
      "outputs": [],
      "source": [
        "# Exploración de parámetros de decodificación\n",
        "from typing import Dict, Any\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "consulta: str = \"Escribe una analogía breve para explicar RAG a un público no técnico.\"\n",
        "\n",
        "configuraciones: Dict[str, Dict[str, Any]] = {\n",
        "    \"baseline\":   {\"temperature\": 0.7, \"top_p\": 0.95, \"repetition_penalty\": 1.0, \"max_new_tokens\": 128},\n",
        "    \"creativo\":   {\"temperature\": 1.1, \"top_p\": 0.90, \"repetition_penalty\": 1.0, \"max_new_tokens\": 128},\n",
        "    \"controlado\": {\"temperature\": 0.2, \"top_p\": 0.80, \"repetition_penalty\": 1.1, \"max_new_tokens\": 96},\n",
        "}\n",
        "\n",
        "for nombre, cfg in configuraciones.items():\n",
        "    tmp_endpoint = HuggingFaceEndpoint(\n",
        "        repo_id=MODEL_ID,\n",
        "        task=\"conversational\",\n",
        "        huggingfacehub_api_token=HF_TOKEN,\n",
        "        temperature=cfg[\"temperature\"],\n",
        "        top_p=cfg[\"top_p\"],\n",
        "        repetition_penalty=cfg[\"repetition_penalty\"],\n",
        "        max_new_tokens=cfg[\"max_new_tokens\"],\n",
        "    )\n",
        "    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n",
        "    salida = (prompt | tmp_llm | StrOutputParser()).invoke({\"instruccion\": consulta})\n",
        "    print(f\"\\n==== {nombre} ({cfg}) ====\\n{salida}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV21ITSqzNyq"
      },
      "source": [
        "### Análisis de las configuraciones\n",
        "\n",
        "#### 1. Baseline (Equilibrado)\n",
        "- `temperature: 0.7` (Moderada)\n",
        "- `top_p: 0.95` (Alto)\n",
        "- `max_new_tokens: 128` (Largo estándar)\n",
        "\n",
        "Esta configuración busca un **equilibrio entre coherencia y creatividad**.\n",
        "- La **temperatura (0.7)** es moderada, permitiendo que el modelo sea un poco creativo sin desviarse demasiado de las respuestas más probables o \"seguras\"\n",
        "- El **top_p (0.95)** le da un amplio \"menú\" de palabras posibles para elegir, fomentando la variedad\n",
        "\n",
        "**Resultado:** La respuesta es clara, usa una buena analogía (\"asistente que busca la mejor información en tiempo real\") y es informativa. Es la respuesta estándar y más fiable.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Creativo (Exploratorio)\n",
        "- `temperature: 1.1` (Alta)\n",
        "- `top_p: 0.9` (Alto)\n",
        "- `max_new_tokens: 128` (Largo estándar)\n",
        "\n",
        "El factor clave aquí es la **alta temperatura (1.1)**.\n",
        "- Una temperatura superior a 1.0 **aumenta la aleatoriedad**. Fuerza al modelo a considerar palabras menos probables, lo que puede llevar a respuestas más originales o inesperadas\n",
        "- El `top_p` sigue siendo alto, dándole muchas opciones\n",
        "\n",
        "**Resultado:** La analogía es diferente y más específica que la del baseline. En lugar de un \"libro de conocimientos\" genérico, crea un escenario más vívido: \"un libro enorme con millones de páginas\", \"encontrar el nombre de un personaje\" y \"preguntas a alguien que conoce bien el contenido\". Es más narrativo y menos directo.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Controlado (Conciso y Determinado)\n",
        "- `temperature: 0.2` (Muy baja)\n",
        "- `top_p: 0.8` (Bajo)\n",
        "- `repetition_penalty: 1.1` (Ligera penalización)\n",
        "- `max_new_tokens: 96` (Corto)\n",
        "\n",
        "Esta configuración está diseñada para ser **precisa, predecible y concisa**.\n",
        "- La **temperatura (0.2)** y el **top_p (0.8)** son bajos. Esto \"enfría\" al modelo, obligándolo a elegir casi siempre la palabra *más probable* y limitando su \"menú\" de opciones. El resultado es menos creativo pero muy enfocado y coherente\n",
        "- La **penalización por repetición (1.1)** evita que se quede atascado repitiendo las mismas palabras \"seguras\" una y otra vez\n",
        "- Los **tokens máximos (96)** fuerzan una respuesta más corta\n",
        "\n",
        "**Resultado:** La respuesta es la más directa y \"seca\". Va al grano (\"busca información relevante... y luego usa esa información\"), usa un ejemplo concreto (\"¿Qué sabes sobre los efectos...?\") y es notablemente más corta que las otras dos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFWQ7vryrB0C"
      },
      "source": [
        "## Parte 2: Prompt Engineering Avanzado\n",
        "\n",
        "Explorarás estrategias para mejorar la calidad y control de las respuestas: zero-shot, few-shot, restricciones de estilo, salida estructurada y Chain of Thought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1U5MiS1rB0C"
      },
      "source": [
        "### Conceptos clave: Zero-shot, Few-shot, CoT y Roles\n",
        "\n",
        "- **Zero-shot**: el modelo resuelve la tarea solo con instrucciones; no se proporcionan ejemplos\n",
        "- **Few-shot**: además de la instrucción, se incluyen 1-3 ejemplos que muestran el formato y estilo deseados\n",
        "- **Chain-of-Thought (CoT)**: se guía al modelo para mostrar pasos intermedios de razonamiento (por ejemplo, \"razona paso a paso\") antes de una respuesta final\n",
        "- **Role prompting**: se asigna un rol (por ejemplo, \"actúa como profesor de IA\") para influir en el estilo y nivel de detalle\n",
        "- **JSON output**: se pide una salida estricta en formato JSON y se valida con un parser\n",
        "\n",
        "**Lectura recomendada**: guía de prompt engineering avanzada en https://learnprompting.org/docs/introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiF9jJRfrB0C"
      },
      "source": [
        "### Zero-shot y Few-shot\n",
        "\n",
        "Compararás la efectividad de proporcionar ejemplos versus no proporcionarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNZ9KnrLrB0D"
      },
      "outputs": [],
      "source": [
        "# Zero-shot vs Few-shot (diferencia marcada con patrón acróstico)\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM más determinista\n",
        "det_endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=MODEL_ID, task=\"conversational\",\n",
        "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=128\n",
        ")\n",
        "llm_det = ChatHuggingFace(llm=det_endpoint)\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Patrón no obvio: acróstico O-V-E (Overfitting), cada línea empieza con esa letra\n",
        "instruccion = (\n",
        "    \"Escribe EXACTAMENTE 3 líneas sobre 'overfitting'. \"\n",
        "    \"Cada línea debe comenzar con O, luego V, luego E (en ese orden). \"\n",
        "    \"6-10 palabras por línea. Sin texto extra.\"\n",
        ")\n",
        "\n",
        "zero_shot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Sigue estrictamente las instrucciones del usuario.\"),\n",
        "    (\"human\", \"{instruccion}\")\n",
        "])\n",
        "\n",
        "few_shot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Sigue estrictamente las instrucciones del usuario.\"),\n",
        "    # Ejemplo: el patrón se demuestra con otro tema y otro acróstico (R-E-G)\n",
        "    (\"human\", \"Escribe EXACTAMENTE 3 líneas sobre 'regularización'. \"\n",
        "              \"Cada línea debe comenzar con R, luego E, luego G. \"\n",
        "              \"6-10 palabras por línea. Sin texto extra.\"),\n",
        "    (\"ai\", \"R Reduce complejidad para evitar ajustes al ruido\\n\"\n",
        "           \"E Estabiliza el aprendizaje con penalizaciones adecuadas\\n\"\n",
        "           \"G Generaliza mejor limitando pesos excesivamente grandes\"),\n",
        "    # Ahora se pide el caso real con el acróstico O-V-E\n",
        "    (\"human\", \"{instruccion}\")\n",
        "])\n",
        "\n",
        "print(\"=== ZERO-SHOT ===\")\n",
        "print((zero_shot | llm_det | parser).invoke({\"instruccion\": instruccion}))\n",
        "\n",
        "print(\"\\n=== FEW-SHOT ===\")\n",
        "print((few_shot | llm_det | parser).invoke({\"instruccion\": instruccion}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b7hTF4wrB0D"
      },
      "source": [
        "### Role Prompting\n",
        "\n",
        "Asignarás un rol específico al modelo para influir en el estilo y nivel de detalle de la respuesta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_VhEgQZrB0D"
      },
      "outputs": [],
      "source": [
        "# Role prompting\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "role_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Actúa como profesor de IA de nivel intermedio. Sé claro, estructurado y usa ejemplos sencillos.\"),\n",
        "    (\"human\", \"Explica brevemente qué es el aprendizaje por refuerzo y menciona 2 ejemplos de aplicación.\"),\n",
        "])\n",
        "\n",
        "print((role_prompt | llm | StrOutputParser()).invoke({}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFhoZDI4rB0D"
      },
      "source": [
        "### CoT (Chain-of-Thought) Prompting\n",
        "\n",
        "Guiarás al modelo para mostrar pasos intermedios de razonamiento antes de dar una respuesta final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AK2aANwrB0D"
      },
      "outputs": [],
      "source": [
        "# Prompts SIN CoT y CON CoT (Bayes) — nscale-compatible\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "problema = (\n",
        "    \"En una población, 1% tiene la enfermedad. La prueba tiene 90% de sensibilidad y 90% de especificidad. \"\n",
        "    \"Si una persona da positivo, ¿cuál es la probabilidad (en %) de que realmente esté enferma?\"\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# SIN CoT: SOLO porcentaje en una línea (recorta tokens)\n",
        "endpoint_sin = HuggingFaceEndpoint(\n",
        "    repo_id=MODEL_ID, task=\"conversational\",\n",
        "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=6\n",
        ")\n",
        "llm_sin = ChatHuggingFace(llm=endpoint_sin)\n",
        "\n",
        "prompt_sin_cot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Devuelve SOLO un número en formato porcentaje (ej: 8.33%). \"\n",
        "               \"Sin explicaciones, sin ecuaciones, sin texto extra.\"),\n",
        "    (\"human\", \"{q}\")\n",
        "])\n",
        "\n",
        "# CON CoT: piensa paso a paso y cierra con una línea final\n",
        "endpoint_con = HuggingFaceEndpoint(\n",
        "    repo_id=MODEL_ID, task=\"conversational\",\n",
        "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=256\n",
        ")\n",
        "llm_con = ChatHuggingFace(llm=endpoint_con)\n",
        "\n",
        "prompt_con_cot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Tómate tu tiempo y piensa paso a paso usando Bayes. \"\n",
        "               \"Al final, da una sola línea con: 'Respuesta final: <n>%'\"),\n",
        "    (\"human\", \"{q}\")\n",
        "])\n",
        "\n",
        "print(\"=== SIN CoT ===\")\n",
        "print((prompt_sin_cot | llm_sin | parser).invoke({\"q\": problema}))\n",
        "\n",
        "print(\"\\n=== CON CoT ===\")\n",
        "print((prompt_con_cot | llm_con | parser).invoke({\"q\": problema}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SPYi9X1rB0F"
      },
      "source": [
        "### Salida estructurada (JSON Output Parser)\n",
        "\n",
        "Solicitarás al modelo una salida en formato JSON estricto y la validarás con un parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCegKKRDrB0F"
      },
      "outputs": [],
      "source": [
        "# Salida estructurada (JSON) con output parser\n",
        "import json\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "json_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Eres un asistente que devuelve SIEMPRE JSON válido. Dado un tema, devuelve un objeto con:\n",
        "    - \"titulo\": string\n",
        "    - \"puntos_clave\": lista de 3 strings\n",
        "    - \"dificultad\": uno de [\"básico\", \"intermedio\", \"avanzado\"]\n",
        "    Responde SOLO con JSON válido sin texto adicional.\n",
        "    Tema: {tema}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "json_text = (json_prompt | llm | StrOutputParser()).invoke({\"tema\": \"RAG\"})\n",
        "print(json_text)\n",
        "\n",
        "data = json.loads(json_text)\n",
        "assert set([\"titulo\", \"puntos_clave\", \"dificultad\"]).issubset(data.keys())\n",
        "print(\"JSON válido con las claves requeridas.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC4DngMVrB0G"
      },
      "source": [
        "## Ejercicios: Parte 2\n",
        "\n",
        "Resuelve los siguientes ejercicios. Modifica prompts y parámetros si es necesario y justifica brevemente tus decisiones (en una celda de texto)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2NZm-Md2Jy4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFAmf9D0rB0G"
      },
      "source": [
        "### Ejercicio 2.1: Zero-shot vs Few-shot\n",
        "\n",
        "**Tarea**: explicar \"regularización L2\" en 3 viñetas claras para un público técnico.\n",
        "\n",
        "**Pasos**:\n",
        "1. **Zero-shot**: crea un prompt sin ejemplos y observa el resultado\n",
        "2. **Few-shot**: agrega 1-2 ejemplos de estilo y compara la salida\n",
        "\n",
        "**Pregunta guía**: ¿mejoró la precisión o claridad con pocos ejemplos? Justifica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd2vCaKlrB0G"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM más determinista\n",
        "det_endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=MODEL_ID, task=\"conversational\",\n",
        "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=128\n",
        ")\n",
        "llm_det = ChatHuggingFace(llm=det_endpoint)\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "def zero_shot_l2(instruccion: str) -> str:\n",
        "    # TODO: construir prompt zero-shot (3 viñetas) y llamar al LLM\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Eres un asistente útil para un público técnico. Explica el siguiente concepto en 3 viñetas claras.\"),\n",
        "        (\"human\", \"Explica '{instruccion}'\")\n",
        "    ])\n",
        "    chain = prompt | llm_det | parser\n",
        "    return chain.invoke({\"instruccion\": instruccion})\n",
        "\n",
        "\n",
        "def few_shot_l2(instruccion: str) -> str:\n",
        "    # TODO: construir prompt con 1-2 ejemplos y llamar al LLM\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Eres un asistente útil para un público técnico. Explica el siguiente concepto en 3 viñetas claras.\"),\n",
        "        (\"human\", \"Explica 'Dropout'\"),\n",
        "        (\"ai\", \"- Desactiva neuronas aleatoriamente durante el entrenamiento.\\n- Reduce el co-adaptación de las neuronas.\\n- Ayuda a prevenir el overfitting.\"),\n",
        "        (\"human\", \"Explica '{instruccion}'\")\n",
        "    ])\n",
        "    chain = prompt | llm_det | parser\n",
        "    return chain.invoke({\"instruccion\": instruccion})\n",
        "\n",
        "instruccion_l2 = \"regularización L2\"\n",
        "\n",
        "print(\"\\n=== ZERO-SHOT ===\\n\")\n",
        "print(zero_shot_l2(instruccion_l2))\n",
        "\n",
        "print(\"\\n=== FEW-SHOT ===\\n\")\n",
        "print(few_shot_l2(instruccion_l2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsrH2ZdtrB0G"
      },
      "source": [
        "### Ejercicio 2.2: Chain-of-Thought (CoT)\n",
        "\n",
        "**Tarea**: dado un problema de evaluación de modelos, razonar paso a paso y entregar una conclusión final breve.\n",
        "\n",
        "**Problema sugerido**: \"¿Por qué accuracy puede ser engañoso en un dataset muy desbalanceado y qué métrica alternativa usarías?\"\n",
        "\n",
        "**Pista**: pide explícitamente \"razona paso a paso y luego da una respuesta final breve en una línea\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37-CDC6irB0G"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def cot_razonamiento(problema: str) -> str:\n",
        "    # TODO: pedir \"razona paso a paso\" y cerrar con una línea final\n",
        "    endpoint_con = HuggingFaceEndpoint(\n",
        "        repo_id=MODEL_ID, task=\"conversational\",\n",
        "        huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=256\n",
        "    )\n",
        "    llm_con = ChatHuggingFace(llm=endpoint_con)\n",
        "\n",
        "    prompt_con_cot = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Tómate tu tiempo y piensa paso a paso. Al final, da una sola línea con: 'Respuesta final: <n>'\"),\n",
        "        (\"human\", \"{q}\")\n",
        "    ])\n",
        "    chain = prompt_con_cot | llm_con | StrOutputParser()\n",
        "    return chain.invoke({\"q\": problema})\n",
        "\n",
        "problema: str = \"¿Por qué accuracy puede ser engañoso en un dataset muy desbalanceado y qué métrica alternativa usarías?\"\n",
        "print(cot_razonamiento(problema))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRDMh9ewrB0G"
      },
      "source": [
        "### Ejercicio 2.3: Role Prompting\n",
        "\n",
        "**Tarea**: explicar el \"sesgo de selección\" a un equipo de data engineering con ejemplos concisos.\n",
        "\n",
        "**Rol**: \"Actúa como líder técnico de datos; sé pragmático y directo, con viñetas concretas\".\n",
        "\n",
        "**Objetivo**: evaluar cómo cambia el estilo bajo un rol técnico específico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnGf_iFWrB0G"
      },
      "outputs": [],
      "source": [
        "# Role prompting\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def explicar_sesgo_seleccion() -> str:\n",
        "    # TODO: role \"líder técnico de datos\", 3 viñetas + 1 ejemplo práctico\n",
        "    role_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Actúa como líder técnico de datos; sé pragmático y directo, con viñetas concretas.\"),\n",
        "    (\"human\", \"Explica brevemente qué es el sesgo de selección y da un ejemplo práctico.\"),\n",
        "    ])\n",
        "\n",
        "    chain = role_prompt | llm | StrOutputParser()\n",
        "    return chain.invoke({})\n",
        "\n",
        "print(explicar_sesgo_seleccion())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

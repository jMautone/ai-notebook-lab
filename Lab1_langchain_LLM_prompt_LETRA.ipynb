{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBCSNaCdrBz-"
      },
      "source": [
        "# Laboratorio: Setup y uso básico de LLMs con LangChain + Prompt engineering avanzado\n",
        "\n",
        "Este laboratorio está pensado para completarse en ~2 horas. Trabajaremos en Google Colab con recursos gratuitos, usando la Inference API de Hugging Face y un modelo instruct abierto.\n",
        "\n",
        "- Contenidos:\n",
        "  - Setup en Colab y configuración de Hugging Face Inference API\n",
        "  - Uso básico de LLMs con LangChain (LCEL)\n",
        "  - Parámetros de decodificación y control de estilo\n",
        "  - Prompt engineering avanzado: zero-shot, few-shot, Chain of Thought, Role Prompting y salida estructurada (JSON)\n",
        "\n",
        "Al finalizar, podrás:\n",
        "- Conectarte a un LLM instruct vía Hugging Face Inference API desde LangChain.\n",
        "- Construir cadenas simples con `prompt | llm | parser`.\n",
        "- Diseñar prompts efectivos y controlar formato de salida (incluido JSON).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RTG7_ybrBz_"
      },
      "source": [
        "## Parte 0 — Setup (Colab + librerías + token)\n",
        "\n",
        "Usaremos versiones estables para minimizar fricción en Colab. Asegurate de ejecutar esta sección antes de continuar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxoJSkLcrBz_"
      },
      "outputs": [],
      "source": [
        "# Instalar dependencias principales (versiones estables)\n",
        "!pip -q install -U \\\n",
        "  \"langchain==0.3.27\" \\\n",
        "  \"langchain-community==0.3.27\" \\\n",
        "  \"langchain-huggingface==0.3.1\" \\\n",
        "  \"transformers==4.55.2\" \\\n",
        "  \"huggingface_hub==0.34.4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmVbT5uRxn5R"
      },
      "outputs": [],
      "source": [
        "from importlib.metadata import version, PackageNotFoundError\n",
        "\n",
        "for dist in [\"langchain\", \"langchain-community\", \"langchain-huggingface\",\n",
        "             \"transformers\", \"huggingface_hub\"]:\n",
        "    try:\n",
        "        print(dist, \"=>\", version(dist))\n",
        "    except PackageNotFoundError:\n",
        "        print(dist, \"no instalado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9EJxPOCrB0A"
      },
      "outputs": [],
      "source": [
        "# Comprobar versión de Python y GPU/CPU\n",
        "import sys, subprocess, torch\n",
        "print(sys.version)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"PyTorch:\", torch.__version__)\n",
        "    print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"PyTorch no disponible o sin CUDA\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ug4tn-VrB0A"
      },
      "source": [
        "### Configuración del token de Hugging Face\n",
        "\n",
        "Para usar la Hugging Face Inference API, necesitás un token personal (gratuito). En Colab se recomienda guardarlo en `userdata`:\n",
        "\n",
        "1. Crear el token en `https://huggingface.co/settings/tokens`.\n",
        "2. En Colab: Abre el menú en la barra izquierda haciendo click en la llave → \"Agregar nuevo secreto\" ponle `HF_TOKEN` y el token que generamos → Habilita el acceso al notebook.\n",
        "3. Ejecutar la celda siguiente para leerlo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMTYDzh9rB0A"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "assert HF_TOKEN is not None and len(HF_TOKEN) > 0, \"Configurar el secreto 'HF_TOKEN' en Colab.\"\n",
        "print(\"Token cargado OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQbt6t3FrB0A"
      },
      "source": [
        "## Parte 1 — Uso básico de LLMs con LangChain\n",
        "\n",
        "Trabajaremos con un modelo instruct accesible vía Inference API. Para minimizar fricción, usaremos un modelo abierto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNzk__gZrB0B"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "\n",
        "hf_endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=MODEL_ID,\n",
        "    task=\"conversational\",\n",
        "    huggingfacehub_api_token=HF_TOKEN,\n",
        "    temperature=0.7,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "\n",
        "llm = ChatHuggingFace(llm=hf_endpoint)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Eres un asistente útil y conciso.\"),\n",
        "    (\"human\", \"Responde a la siguiente instrucción: {instruccion}\"),\n",
        "])\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "print(chain.invoke({\"instruccion\": \"Explica en 3 frases qué es un LLM y nombra 2 casos de uso.\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM0fs9ZLrB0B"
      },
      "source": [
        "### Ejercicio 1.1 (10 min)\n",
        "\n",
        "- Probar 3 variaciones de `temperature` y observar el cambio en estilo.\n",
        "- Cambiar el rol del `system` para forzar un estilo (p.ej., “responde con viñetas y máximo 3 líneas”).\n",
        "- Pregunta sugerida: “Resume la diferencia entre entrenamiento y fine-tuning en 3 puntos.”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcTYMM-srB0B",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "instruccion: str = \"Resume la diferencia entre entrenamiento y fine-tuning en 3 puntos.\"\n",
        "temperaturas: List[float] = [0.0, 0.7, 1.2]\n",
        "\n",
        "def ejecutar_variacion(temperature: float) -> str:\n",
        "    # TODO: crear endpoint conversacional con 'temperature' y devolver el texto\n",
        "    # Debe usar: MODEL_ID, HF_TOKEN, task=\"conversational\"\n",
        "    tmp_endpoint = HuggingFaceEndpoint(\n",
        "        repo_id=MODEL_ID,\n",
        "        task=\"conversational\",\n",
        "        huggingfacehub_api_token=HF_TOKEN,\n",
        "        temperature=temperature,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Eres un asistente útil y conciso.\"),\n",
        "        (\"human\", \"Responde a la siguiente instrucción: {instruccion}\"),\n",
        "    ])\n",
        "    chain = prompt | tmp_llm | StrOutputParser()\n",
        "    return chain.invoke({\"instruccion\": instruccion})\n",
        "\n",
        "\n",
        "for t in temperaturas:\n",
        "    print(f\"\\n==== temperature: {t} ====\")\n",
        "    print(ejecutar_variacion(t))\n",
        "\n",
        "def ejecutar_estilo(instruccion: str) -> str:\n",
        "    # TODO: crear prompt de estilo (system) + LLM base conversacional y devolver el texto\n",
        "    styled_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Responde con viñetas y máximo 3 líneas.\"),\n",
        "        (\"human\", \"Responde a la siguiente instrucción: {instruccion}\"),\n",
        "    ])\n",
        "    chain = styled_prompt | llm | StrOutputParser()\n",
        "    return chain.invoke({\"instruccion\": instruccion})\n",
        "\n",
        "\n",
        "print(\"\\n==== estilo forzado ====\")\n",
        "print(ejecutar_estilo(instruccion))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi_Cg8IMrB0C"
      },
      "source": [
        "## Parte 1.2 — Parámetros de decodificación\n",
        "\n",
        "Ajustaremos parámetros como `top_p`, `repetition_penalty` y `max_new_tokens` para observar su efecto en la generación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lCnXq_QrB0C"
      },
      "outputs": [],
      "source": [
        "# Exploración de parámetros de decodificación\n",
        "from typing import Dict, Any\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "consulta: str = \"Escribe una analogía breve para explicar RAG a un público no técnico.\"\n",
        "\n",
        "configuraciones: Dict[str, Dict[str, Any]] = {\n",
        "    \"baseline\":   {\"temperature\": 0.7, \"top_p\": 0.95, \"repetition_penalty\": 1.0, \"max_new_tokens\": 128},\n",
        "    \"creativo\":   {\"temperature\": 1.1, \"top_p\": 0.90, \"repetition_penalty\": 1.0, \"max_new_tokens\": 128},\n",
        "    \"controlado\": {\"temperature\": 0.2, \"top_p\": 0.80, \"repetition_penalty\": 1.1, \"max_new_tokens\": 96},\n",
        "}\n",
        "\n",
        "for nombre, cfg in configuraciones.items():\n",
        "    tmp_endpoint = HuggingFaceEndpoint(\n",
        "        repo_id=MODEL_ID,\n",
        "        task=\"conversational\",\n",
        "        huggingfacehub_api_token=HF_TOKEN,\n",
        "        temperature=cfg[\"temperature\"],\n",
        "        top_p=cfg[\"top_p\"],\n",
        "        repetition_penalty=cfg[\"repetition_penalty\"],\n",
        "        max_new_tokens=cfg[\"max_new_tokens\"],\n",
        "    )\n",
        "    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n",
        "    salida = (prompt | tmp_llm | StrOutputParser()).invoke({\"instruccion\": consulta})\n",
        "    print(f\"\\n==== {nombre} ({cfg}) ====\\n{salida}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. Baseline (Equilibrado)\n",
        "* `'temperature': 0.7` (Moderada)\n",
        "* `'top_p': 0.95` (Alto)\n",
        "* `'max_new_tokens': 128` (Largo estándar)\n",
        "\n",
        "Esta configuración busca un **equilibrio entre coherencia y creatividad**.\n",
        "* La **temperatura (0.7)** es moderada, permitiendo que el modelo sea un poco creativo sin desviarse demasiado de las respuestas más probables o \"seguras\".\n",
        "* El **top_p (0.95)** le da un amplio \"menú\" de palabras posibles para elegir, fomentando la variedad.\n",
        "\n",
        "**Resultado:** La respuesta es clara, usa una buena analogía (\"asistente que busca la mejor información en tiempo real\") y es informativa. Es la respuesta estándar y más fiable.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Creativo (Exploratorio)\n",
        "* `'temperature': 1.1` (Alta)\n",
        "* `'top_p': 0.9` (Alto)\n",
        "* `'max_new_tokens': 128` (Largo estándar)\n",
        "\n",
        "El factor clave aquí es la **alta temperatura (1.1)**.\n",
        "* Una temperatura superior a 1.0 **aumenta la aleatoriedad**. Fuerza al modelo a considerar palabras menos probables, lo que puede llevar a respuestas más originales o inesperadas.\n",
        "* El `top_p` sigue siendo alto, dándole muchas opciones.\n",
        "\n",
        "**Resultado:** La analogía es diferente y más específica que la del baseline. En lugar de un \"libro de conocimientos\" genérico, crea un escenario más vívido: \"un libro enorme con millones de páginas\", \"encontrar el nombre de un personaje\" y \"preguntas a alguien que conoce bien el contenido\". Es más narrativo y menos directo.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Controlado (Conciso y Determinado)\n",
        "* `'temperature': 0.2` (Muy baja)\n",
        "* `'top_p': 0.8` (Bajo)\n",
        "* `'repetition_penalty': 1.1` (Ligera penalización)\n",
        "* `'max_new_tokens': 96` (Corto)\n",
        "\n",
        "Esta configuración está diseñada para ser **precisa, predecible y concisa**.\n",
        "* La **temperatura (0.2)** y el **top_p (0.8)** son bajos. Esto \"enfría\" al modelo, obligándolo a elegir casi siempre la palabra *más probable* y limitando su \"menú\" de opciones. El resultado es menos creativo pero muy enfocado y coherente.\n",
        "* La **penalización por repetición (1.1)** evita que se quede atascado repitiendo las mismas palabras \"seguras\" una y otra vez.\n",
        "* Los **tokens máximos (96)** fuerzan una respuesta más corta.\n",
        "\n",
        "**Resultado:** La respuesta es la más directa y \"seca\". Va al grano (\"busca información relevante... y luego usa esa información\"), usa un ejemplo concreto (\"¿Qué sabes sobre los efectos...?\") y es notablemente más corta que las otras dos."
      ],
      "metadata": {
        "id": "WV21ITSqzNyq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFWQ7vryrB0C"
      },
      "source": [
        "## Parte 2 — Prompt engineering avanzado\n",
        "\n",
        "Exploraremos estrategias para mejorar la calidad y control de las respuestas: zero-shot, few-shot, restricciones de estilo, salida estructurada y Chain of Thought.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1U5MiS1rB0C"
      },
      "source": [
        "### Conceptos clave: Zero-shot, Few-shot, CoT y Roles\n",
        "\n",
        "- Zero-shot: el modelo resuelve la tarea solo con instrucciones; no se proporcionan ejemplos.\n",
        "- Few-shot: además de la instrucción, se incluyen 1-3 ejemplos que muestran el formato y estilo deseados.\n",
        "- Chain-of-Thought (CoT): se guía al modelo para mostrar pasos intermedios de razonamiento (p.ej., “razona paso a paso”) antes de una respuesta final.\n",
        "- Role prompting: se asigna un rol (p.ej., “actúa como profesor de IA”) para influir en el estilo y nivel de detalle.\n",
        "- JSON output: se pide una salida estricta en formato JSON y se valida con un parser.\n",
        "\n",
        "Lectura recomendada: guía de prompt engineering avanzada en `https://learnprompting.org/docs/introduction`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiF9jJRfrB0C"
      },
      "source": [
        "### Zero-shot y Few-shot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNZ9KnrLrB0D"
      },
      "outputs": [],
      "source": [
        "# Zero-shot vs Few-shot (diferencia marcada con patrón acróstico)\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM más determinista\n",
        "det_endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=MODEL_ID, task=\"conversational\",\n",
        "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=128\n",
        ")\n",
        "llm_det = ChatHuggingFace(llm=det_endpoint)\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Patrón no obvio: acróstico O-V-E (Overfitting), cada línea empieza con esa letra\n",
        "instruccion = (\n",
        "    \"Escribe EXACTAMENTE 3 líneas sobre 'overfitting'. \"\n",
        "    \"Cada línea debe comenzar con O, luego V, luego E (en ese orden). \"\n",
        "    \"6-10 palabras por línea. Sin texto extra.\"\n",
        ")\n",
        "\n",
        "zero_shot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Sigue estrictamente las instrucciones del usuario.\"),\n",
        "    (\"human\", \"{instruccion}\")\n",
        "])\n",
        "\n",
        "few_shot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Sigue estrictamente las instrucciones del usuario.\"),\n",
        "    # Ejemplo: el patrón se demuestra con otro tema y otro acróstico (R-E-G)\n",
        "    (\"human\", \"Escribe EXACTAMENTE 3 líneas sobre 'regularización'. \"\n",
        "              \"Cada línea debe comenzar con R, luego E, luego G. \"\n",
        "              \"6-10 palabras por línea. Sin texto extra.\"),\n",
        "    (\"ai\", \"R Reduce complejidad para evitar ajustes al ruido\\n\"\n",
        "           \"E Estabiliza el aprendizaje con penalizaciones adecuadas\\n\"\n",
        "           \"G Generaliza mejor limitando pesos excesivamente grandes\"),\n",
        "    # Ahora se pide el caso real con el acróstico O-V-E\n",
        "    (\"human\", \"{instruccion}\")\n",
        "])\n",
        "\n",
        "print(\"=== ZERO-SHOT ===\")\n",
        "print((zero_shot | llm_det | parser).invoke({\"instruccion\": instruccion}))\n",
        "\n",
        "print(\"\\n=== FEW-SHOT ===\")\n",
        "print((few_shot | llm_det | parser).invoke({\"instruccion\": instruccion}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b7hTF4wrB0D"
      },
      "source": [
        "### Role prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_VhEgQZrB0D"
      },
      "outputs": [],
      "source": [
        "# Role prompting\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "role_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Actúa como profesor de IA de nivel intermedio. Sé claro, estructurado y usa ejemplos sencillos.\"),\n",
        "    (\"human\", \"Explica brevemente qué es el aprendizaje por refuerzo y menciona 2 ejemplos de aplicación.\"),\n",
        "])\n",
        "\n",
        "print((role_prompt | llm | StrOutputParser()).invoke({}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFhoZDI4rB0D"
      },
      "source": [
        "### CoT (Chain-of-Thought) prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AK2aANwrB0D"
      },
      "outputs": [],
      "source": [
        "# Prompts SIN CoT y CON CoT (Bayes) — nscale-compatible\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "problema = (\n",
        "    \"En una población, 1% tiene la enfermedad. La prueba tiene 90% de sensibilidad y 90% de especificidad. \"\n",
        "    \"Si una persona da positivo, ¿cuál es la probabilidad (en %) de que realmente esté enferma?\"\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# SIN CoT: SOLO porcentaje en una línea (recorta tokens)\n",
        "endpoint_sin = HuggingFaceEndpoint(\n",
        "    repo_id=MODEL_ID, task=\"conversational\",\n",
        "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=6\n",
        ")\n",
        "llm_sin = ChatHuggingFace(llm=endpoint_sin)\n",
        "\n",
        "prompt_sin_cot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Devuelve SOLO un número en formato porcentaje (ej: 8.33%). \"\n",
        "               \"Sin explicaciones, sin ecuaciones, sin texto extra.\"),\n",
        "    (\"human\", \"{q}\")\n",
        "])\n",
        "\n",
        "# CON CoT: piensa paso a paso y cierra con una línea final\n",
        "endpoint_con = HuggingFaceEndpoint(\n",
        "    repo_id=MODEL_ID, task=\"conversational\",\n",
        "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=256\n",
        ")\n",
        "llm_con = ChatHuggingFace(llm=endpoint_con)\n",
        "\n",
        "prompt_con_cot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Tómate tu tiempo y piensa paso a paso usando Bayes. \"\n",
        "               \"Al final, da una sola línea con: 'Respuesta final: <n>%'\"),\n",
        "    (\"human\", \"{q}\")\n",
        "])\n",
        "\n",
        "print(\"=== SIN CoT ===\")\n",
        "print((prompt_sin_cot | llm_sin | parser).invoke({\"q\": problema}))\n",
        "\n",
        "print(\"\\n=== CON CoT ===\")\n",
        "print((prompt_con_cot | llm_con | parser).invoke({\"q\": problema}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SPYi9X1rB0F"
      },
      "source": [
        "### Salida estructurada (JSON Output Parser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCegKKRDrB0F"
      },
      "outputs": [],
      "source": [
        "# Salida estructurada (JSON) con output parser\n",
        "import json\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "json_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Eres un asistente que devuelve SIEMPRE JSON válido. Dado un tema, devuelve un objeto con:\n",
        "    - \"titulo\": string\n",
        "    - \"puntos_clave\": lista de 3 strings\n",
        "    - \"dificultad\": uno de [\"básico\", \"intermedio\", \"avanzado\"]\n",
        "    Responde SOLO con JSON válido sin texto adicional.\n",
        "    Tema: {tema}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "json_text = (json_prompt | llm | StrOutputParser()).invoke({\"tema\": \"RAG\"})\n",
        "print(json_text)\n",
        "\n",
        "data = json.loads(json_text)\n",
        "assert set([\"titulo\", \"puntos_clave\", \"dificultad\"]).issubset(data.keys())\n",
        "print(\"JSON válido con las claves requeridas.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC4DngMVrB0G"
      },
      "source": [
        "## Ejercicios — Parte 2\n",
        "\n",
        "Resuelve los siguientes ejercicios. Modifica prompts y parámetros si es necesario y justifica brevemente tus decisiones (en una celda de texto).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "f2NZm-Md2Jy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFAmf9D0rB0G"
      },
      "source": [
        "### Ejercicio 2.1 — Zero-shot vs Few-shot\n",
        "\n",
        "- Tarea: explicar “regularización L2” en 3 viñetas claras para un público técnico.\n",
        "- Paso 1 (zero-shot): crea un prompt sin ejemplos y observa el resultado.\n",
        "- Paso 2 (few-shot): agrega 1-2 ejemplos de estilo y compara la salida.\n",
        "- Pregunta guía: ¿mejoró la precisión o claridad con pocos ejemplos? Justifica.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd2vCaKlrB0G"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM más determinista\n",
        "det_endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=MODEL_ID, task=\"conversational\",\n",
        "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=128\n",
        ")\n",
        "llm_det = ChatHuggingFace(llm=det_endpoint)\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "def zero_shot_l2(instruccion: str) -> str:\n",
        "    # TODO: construir prompt zero-shot (3 viñetas) y llamar al LLM\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Eres un asistente útil para un público técnico. Explica el siguiente concepto en 3 viñetas claras.\"),\n",
        "        (\"human\", \"Explica '{instruccion}'\")\n",
        "    ])\n",
        "    chain = prompt | llm_det | parser\n",
        "    return chain.invoke({\"instruccion\": instruccion})\n",
        "\n",
        "\n",
        "def few_shot_l2(instruccion: str) -> str:\n",
        "    # TODO: construir prompt con 1-2 ejemplos y llamar al LLM\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Eres un asistente útil para un público técnico. Explica el siguiente concepto en 3 viñetas claras.\"),\n",
        "        (\"human\", \"Explica 'Dropout'\"),\n",
        "        (\"ai\", \"- Desactiva neuronas aleatoriamente durante el entrenamiento.\\n- Reduce el co-adaptación de las neuronas.\\n- Ayuda a prevenir el overfitting.\"),\n",
        "        (\"human\", \"Explica '{instruccion}'\")\n",
        "    ])\n",
        "    chain = prompt | llm_det | parser\n",
        "    return chain.invoke({\"instruccion\": instruccion})\n",
        "\n",
        "instruccion_l2 = \"regularización L2\"\n",
        "\n",
        "print(\"\\n=== ZERO-SHOT ===\\n\")\n",
        "print(zero_shot_l2(instruccion_l2))\n",
        "\n",
        "print(\"\\n=== FEW-SHOT ===\\n\")\n",
        "print(few_shot_l2(instruccion_l2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsrH2ZdtrB0G"
      },
      "source": [
        "### Ejercicio 2.2 — Chain-of-Thought (CoT)\n",
        "\n",
        "- Tarea: dado un problema de evaluación de modelos, razonar paso a paso y entregar una conclusión final breve.\n",
        "- Problema sugerido: “¿Por qué accuracy puede ser engañoso en un dataset muy desbalanceado y qué métrica alternativa usarías?”\n",
        "- Pista: pide explícitamente “razona paso a paso y luego da una respuesta final breve en una línea”.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37-CDC6irB0G"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def cot_razonamiento(problema: str) -> str:\n",
        "    # TODO: pedir \"razona paso a paso\" y cerrar con una línea final\n",
        "    endpoint_con = HuggingFaceEndpoint(\n",
        "        repo_id=MODEL_ID, task=\"conversational\",\n",
        "        huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=256\n",
        "    )\n",
        "    llm_con = ChatHuggingFace(llm=endpoint_con)\n",
        "\n",
        "    prompt_con_cot = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Tómate tu tiempo y piensa paso a paso. Al final, da una sola línea con: 'Respuesta final: <n>'\"),\n",
        "        (\"human\", \"{q}\")\n",
        "    ])\n",
        "    chain = prompt_con_cot | llm_con | StrOutputParser()\n",
        "    return chain.invoke({\"q\": problema})\n",
        "\n",
        "problema: str = \"¿Por qué accuracy puede ser engañoso en un dataset muy desbalanceado y qué métrica alternativa usarías?\"\n",
        "print(cot_razonamiento(problema))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRDMh9ewrB0G"
      },
      "source": [
        "### Ejercicio 2.3 — Role prompting\n",
        "\n",
        "- Tarea: explicar el “sesgo de selección” a un equipo de data engineering con ejemplos concisos.\n",
        "- Rol: “Actúa como líder técnico de datos; sé pragmático y directo, con viñetas concretas”.\n",
        "- Objetivo: evaluar cómo cambia el estilo bajo un rol técnico específico.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnGf_iFWrB0G"
      },
      "outputs": [],
      "source": [
        "# Role prompting\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def explicar_sesgo_seleccion() -> str:\n",
        "    # TODO: role \"líder técnico de datos\", 3 viñetas + 1 ejemplo práctico\n",
        "    role_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Actúa como líder técnico de datos; sé pragmático y directo, con viñetas concretas.\"),\n",
        "    (\"human\", \"Explica brevemente qué es el sesgo de selección y da un ejemplo práctico.\"),\n",
        "    ])\n",
        "\n",
        "    chain = role_prompt | llm | StrOutputParser()\n",
        "    return chain.invoke({})\n",
        "\n",
        "print(explicar_sesgo_seleccion())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
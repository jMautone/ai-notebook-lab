{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio 5: RAG Agent"
      ],
      "metadata": {
        "id": "t1k__37zN_EQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c42028e"
      },
      "source": [
        "## Parte 0: Instalación de dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d71d32e"
      },
      "source": [
        "En este laboratorio, explorarás la creación e implementación de un agente de Retrieval Augmented Generation (RAG). Un agente RAG combina la capacidad de un modelo de lenguaje para generar texto con la habilidad de recuperar información relevante de una base de conocimiento externa.\n",
        "\n",
        "A lo largo de este laboratorio, realizarás los siguientes pasos clave:\n",
        "\n",
        "1.  **Implementación de herramientas**: Desarrollarás funciones para procesar, guardar y buscar información en la base vectorial.\n",
        "2.  **Creación de agentes**: Construirás agentes especializados (memoria e investigación).\n",
        "3.  **Implementación del supervisor**: Diseñarás un agente supervisor para dirigir las consultas al agente adecuado.\n",
        "4.  **Orquestación con LangGraph**: Utilizarás LangGraph para definir el flujo de interacción entre los agentes.\n",
        "5.  **Pruebas**: Validarás el funcionamiento del agente RAG con diferentes consultas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalación de dependencias"
      ],
      "metadata": {
        "id": "7AOJch4dTGRC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "c9f7319a"
      },
      "source": [
        "%pip install langchain-openai openai pinecone sentence-transformers wikipedia langgraph langgraph-supervisor langchain_community langchain-core"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1: Inicialización del modelo de lenguaje, embeddings y conexión con Pinecone"
      ],
      "metadata": {
        "id": "e3Qa0LqTe21H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializa un modelo de lenguaje de OpenAI usando la clase ChatOpenAI.\n",
        "\n",
        "Configura un modelo de embeddings de Hugging Face. Usa el modelo \"sentence-transformers/all-MiniLM-L6-v2\".\n",
        "\n",
        "Conéctate a Pinecone utilizando su clave de API desde userdata.get('pinecone_api_key').\n",
        "\n",
        "Crea un índice vectorial en Pinecone llamado \"general-info\" con las siguientes características:\n",
        "- Dimensión: 384\n",
        "- Métrica: \"cosine\"\n",
        "- Especificación de servidor: tipo ServerlessSpec, nube \"aws\", región \"us-east-1\"."
      ],
      "metadata": {
        "id": "TTu4LDzITJep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVpkz9YvI8CU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Celda para Colab: usa los nombres exactos de secrets y crea el índice \"general-info\"\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import os\n",
        "\n",
        "# -------------------------\n",
        "# 1) Leer secrets (NOMBRES EXACTOS)\n",
        "# -------------------------\n",
        "OPENAI_API_TOKEN = userdata.get('OPENAI_API_TOKEN')   # nombre exacto pedido\n",
        "HUGGING_API_KEY     = userdata.get('HUGGING_API_KEY') # nombre exacto pedido\n",
        "PINECONE_API_KEY    = userdata.get('PINECONE_API_KEY')# nombre exacto pedido\n",
        "\n",
        "# Validaciones claras\n",
        "if not OPENAI_API_TOKEN:\n",
        "    raise ValueError(\"Falta 'OPENAI_API_TOKEN' en userdata. Añadí la secret con ese nombre en Colab.\")\n",
        "if not HUGGING_API_KEY:\n",
        "    raise ValueError(\"Falta 'HUGGING_API_KEY' en userdata. Añadí la secret con ese nombre en Colab.\")\n",
        "if not PINECONE_API_KEY:\n",
        "    raise ValueError(\"Falta 'PINECONE_API_KEY' en userdata. Añadí la secret con ese nombre en Colab.\")\n",
        "\n",
        "# Exportar a env vars que algunas librerías esperan\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_TOKEN\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGING_API_KEY\n",
        "\n",
        "# -------------------------\n",
        "# 2) Inicializar ChatOpenAI\n",
        "# -------------------------\n",
        "# Se usa la clase ChatOpenAI tal como pide la consigna.\n",
        "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0, openai_api_key=OPENAI_API_TOKEN)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Inicializar embeddings HF (all-MiniLM-L6-v2)\n",
        "# -------------------------\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "print(f\"Iniciando SentenceTransformer {EMBED_MODEL_NAME} (esto descarga el modelo la 1ra vez)...\")\n",
        "sbert = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "# wrapper de LangChain (opcional para uso con LangChain)\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
        "\n",
        "# -------------------------\n",
        "# 4) Conectar a Pinecone\n",
        "# -------------------------\n",
        "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# Parámetros del índice pedidos por la consigna\n",
        "index_name = \"general-info\"\n",
        "dimension = 384\n",
        "metric = \"cosine\"\n",
        "serverless_spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "\n",
        "# -------------------------\n",
        "# 5) Crear índice: varios intentos para compatibilidad con SDKs distintos\n",
        "# -------------------------\n",
        "existing_indexes = pinecone.list_indexes()\n",
        "if index_name in existing_indexes:\n",
        "    print(f\"Índice '{index_name}' ya existe en Pinecone.\")\n",
        "else:\n",
        "    created = False\n",
        "    last_exception = None\n",
        "\n",
        "    # Intento A: argumento 'serverless' (tu intento original)\n",
        "    try:\n",
        "        pinecone.create_index(name=index_name, dimension=dimension, metric=metric, serverless=serverless_spec)\n",
        "        created = True\n",
        "        print(\"Índice creado usando 'serverless' argument.\")\n",
        "    except TypeError as e:\n",
        "        last_exception = e\n",
        "        # Intento B: argumento 'spec' (algunas versiones usan 'spec' en lugar de 'serverless')\n",
        "        try:\n",
        "            pinecone.create_index(name=index_name, dimension=dimension, metric=metric, spec=serverless_spec)\n",
        "            created = True\n",
        "            print(\"Índice creado usando 'spec' argument.\")\n",
        "        except TypeError as e2:\n",
        "            last_exception = e2\n",
        "            # Intento C: firma posicional básica (nombre, dimensión, métrica) — puede crear índice sin serverless\n",
        "            try:\n",
        "                pinecone.create_index(index_name, dimension, metric)\n",
        "                created = True\n",
        "                print(\"Índice creado usando firma posicional (sin serverless/spec).\")\n",
        "                print(\"ATENCIÓN: en este último caso la configuración ServerlessSpec NO se aplicó; confirma en la consola de Pinecone si necesitas cambiar el tipo de índice.\")\n",
        "            except Exception as e3:\n",
        "                last_exception = e3\n",
        "\n",
        "    if not created:\n",
        "        # Si no se pudo crear, mostramos error informativo con el último exception\n",
        "        raise RuntimeError(\"No se pudo crear el índice con las firmas probadas. Último error:\\n\" + repr(last_exception))\n",
        "\n",
        "# -------------------------\n",
        "# 6) Obtener handle del índice (compatibilidad con distintas versiones)\n",
        "# -------------------------\n",
        "index = None\n",
        "try:\n",
        "    index = pinecone.index(index_name)   # forma común\n",
        "except Exception:\n",
        "    try:\n",
        "        index = pinecone.Index(index_name)  # alternativa de algunas versiones\n",
        "    except Exception as e:\n",
        "        print(\"No se pudo obtener handle del índice con pinecone.index ni pinecone.Index. Error:\", e)\n",
        "        index = None\n",
        "\n",
        "print(\"Hecho. ChatOpenAI inicializado, embeddings cargados y Pinecone listo.\")\n",
        "print(\"Handle del índice obtenido?:\", index is not None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 2: Procesamiento de texto y almacenamiento de embeddings en Pinecone"
      ],
      "metadata": {
        "id": "jwaopzvxfS0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementa la función process_and_upsert_files(data: str) que:\n",
        "- Reciba un texto como entrada.\n",
        "- Genere su embedding con embedding_model.embed_query(data).\n",
        "- Guarde el embedding en Pinecone junto con el texto original dentro de la metadata.\n",
        "- Devuelva un mensaje indicando cuántos fragmentos fueron insertados."
      ],
      "metadata": {
        "id": "Wq-aQtrMTu6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "from typing import Any\n",
        "\n",
        "def process_and_upsert_files(data: str) -> str:\n",
        "    \"\"\"\n",
        "    1) Recibe un texto (data).\n",
        "    2) Genera su embedding usando embedding_model.embed_query(data).\n",
        "       - Si no hay variable `embedding_model` en globals, intenta usar `hf_embeddings`.\n",
        "    3) Inserta en Pinecone el vector junto con la metadata que contiene el texto original.\n",
        "    4) Devuelve un mensaje indicando cuántos fragmentos fueron insertados.\n",
        "    \"\"\"\n",
        "    # Validaciones básicas (esperamos que index e index_name existan en el scope global)\n",
        "    if \"index\" not in globals() or globals().get(\"index\") is None:\n",
        "        raise RuntimeError(\"No se encontró el handle `index`. Asegurate de crear/obtener el índice Pinecone antes.\")\n",
        "    if \"index_name\" not in globals() or globals().get(\"index_name\") is None:\n",
        "        raise RuntimeError(\"No se encontró `index_name` en el entorno. Define index_name = 'general-info' por ejemplo.\")\n",
        "\n",
        "    # Resolver el objeto de embeddings (se requiere método embed_query)\n",
        "    embedding_model = globals().get(\"embedding_model\") or globals().get(\"hf_embeddings\")\n",
        "    if embedding_model is None:\n",
        "        raise RuntimeError(\"No hay un modelo de embeddings disponible (ni 'embedding_model' ni 'hf_embeddings').\")\n",
        "\n",
        "    # 1) Generar embedding usando la API requerida: embed_query\n",
        "    emb = embedding_model.embed_query(data)\n",
        "\n",
        "    # Normalizar al formato lista (Pinecone espera listas JSON-serializables)\n",
        "    try:\n",
        "        emb_list = emb.tolist()  # si es numpy array\n",
        "    except Exception:\n",
        "        emb_list = list(emb)     # si es iterador / lista ya\n",
        "\n",
        "    # 2) Preparar payload para upsert: (id, vector, metadata)\n",
        "    _id = str(uuid4())\n",
        "    metadata = {\"text\": data}  # guardamos el texto original en metadata\n",
        "    upsert_items = [(_id, emb_list, metadata)]\n",
        "\n",
        "    # 3) Upsert en Pinecone (manejando variantes de firma del SDK)\n",
        "    try:\n",
        "        # intento más común\n",
        "        index.upsert(vectors=upsert_items)\n",
        "    except TypeError:\n",
        "        try:\n",
        "            # alternativa posible en otras versiones\n",
        "            index.upsert(items=upsert_items)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Error al upsert en Pinecone: \" + str(e))\n",
        "    except Exception as e:\n",
        "        # otros errores (conectividad, keys, etc.)\n",
        "        raise RuntimeError(\"Error al upsert en Pinecone: \" + str(e))\n",
        "\n",
        "    # 4) Retornar mensaje con cuántos fragmentos se insertaron (aquí 1 por llamada)\n",
        "    inserted = len(upsert_items)\n",
        "    return f\"Insertados {inserted} fragmento(s) en el índice '{globals().get('index_name')}'.\""
      ],
      "metadata": {
        "id": "Ha4zBZADKtVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo rápido de uso (en la celda siguiente, tras ejecutar la función):"
      ],
      "metadata": {
        "id": "V8pyb7MPc4jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msg = process_and_upsert_files(\"La capital de Francia es Paris.\")\n",
        "print(msg)"
      ],
      "metadata": {
        "id": "_k1MnFohc9J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 3: Búsqueda semántica en Pinecone con la función search_and_fetch"
      ],
      "metadata": {
        "id": "rb8ThmxYfZim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementa la función search_and_fetch(query_text: str, top_k: int = 3) que:\n",
        "- Genere un embedding de la consulta.\n",
        "- Busque los top_k resultados más similares en Pinecone usando index.query().\n",
        "- Devuelva los resultados incluyendo la metadata con el texto original."
      ],
      "metadata": {
        "id": "Wy311QnkUEWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def search_and_fetch(query_text: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Genera embedding, valida/normaliza el vector y busca los top_k matches en Pinecone.\n",
        "    Devuelve lista de dicts con keys: 'id', 'score', 'metadata'.\n",
        "    \"\"\"\n",
        "    # 1) Resolver modelo de embeddings\n",
        "    emb_model = globals().get(\"embedding_model\") or globals().get(\"hf_embeddings\") or globals().get(\"sbert\")\n",
        "    if emb_model is None:\n",
        "        raise RuntimeError(\"No hay modelo de embeddings disponible ('embedding_model', 'hf_embeddings' o 'sbert').\")\n",
        "\n",
        "    # 2) Generar embedding (soporta sbert o wrappers con embed_query/embed)\n",
        "    if emb_model is globals().get(\"sbert\"):\n",
        "        q_emb = emb_model.encode([query_text], convert_to_numpy=True)[0]\n",
        "    else:\n",
        "        try:\n",
        "            q_emb = emb_model.embed_query(query_text)\n",
        "        except Exception:\n",
        "            q_emb = emb_model.embed([query_text])[0]\n",
        "\n",
        "    # 3) Normalizar a python floats y validar\n",
        "    q_arr = np.asarray(q_emb).ravel()\n",
        "    q_list = [float(x) for x in q_arr.astype(float).tolist()]\n",
        "\n",
        "    # Diagnóstico mínimo (opcional - coméntalo si no querés prints)\n",
        "    print(\"Embedding length:\", len(q_list), \"| first values:\", q_list[:6])\n",
        "\n",
        "    # Comprobar NaN/Inf y dimensión\n",
        "    if any(np.isnan(x) for x in q_list):\n",
        "        raise ValueError(\"El embedding contiene NaN.\")\n",
        "    if any(np.isinf(x) for x in q_list):\n",
        "        raise ValueError(\"El embedding contiene Inf.\")\n",
        "    expected_dim = globals().get(\"dimension\", 384)\n",
        "    if len(q_list) != expected_dim:\n",
        "        raise ValueError(f\"Dimensión del embedding ({len(q_list)}) distinta de la dimensión esperada ({expected_dim}).\")\n",
        "\n",
        "    # 4) Intentar llamadas a index.query con distintas firmas (primero la que funcionó antes)\n",
        "    last_exc = None\n",
        "    resp = None\n",
        "\n",
        "    try_signatures = [\n",
        "        (\"vector_kw\",   lambda v: index.query(vector=v, top_k=top_k, include_metadata=True)),\n",
        "        (\"queries_kw\",  lambda v: index.query(queries=[v], top_k=top_k, include_metadata=True)),\n",
        "        (\"query_vector_kw\", lambda v: index.query(query_vector=v, top_k=top_k, include_metadata=True)),\n",
        "        (\"positional\",  lambda v: index.query(v, top_k)),\n",
        "        (\"single_vector_kw\", lambda v: index.query(v, top_k=top_k, include_metadata=True)),\n",
        "    ]\n",
        "\n",
        "    for name, fn in try_signatures:\n",
        "        try:\n",
        "            # Intentar la firma\n",
        "            resp = fn(q_list)\n",
        "            print(f\"Query: signature '{name}' OK\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            # Guardar último error y seguir probando\n",
        "            print(f\"Query: signature '{name}' falló -> {type(e).__name__}: {e}\")\n",
        "            last_exc = e\n",
        "            resp = None\n",
        "            continue\n",
        "\n",
        "    if resp is None:\n",
        "        # Ninguna firma funcionó -> relanzar último error (verás el traceback)\n",
        "        raise last_exc\n",
        "\n",
        "    # 5) Normalizar la respuesta de Pinecone a formato consistente\n",
        "    matches_out: List[Dict[str, Any]] = []\n",
        "\n",
        "    # helper para procesar lista de matches (cada match puede ser dict o objeto)\n",
        "    def _process_matches(raw_matches):\n",
        "        out = []\n",
        "        for m in raw_matches:\n",
        "            if isinstance(m, dict):\n",
        "                mid = m.get(\"id\")\n",
        "                mscore = m.get(\"score\")\n",
        "                mmeta = m.get(\"metadata\")\n",
        "            else:\n",
        "                mid = getattr(m, \"id\", None)\n",
        "                mscore = getattr(m, \"score\", None)\n",
        "                mmeta = getattr(m, \"metadata\", None)\n",
        "            out.append({\"id\": mid, \"score\": mscore, \"metadata\": mmeta})\n",
        "        return out\n",
        "\n",
        "    # Caso 1: dict con 'results' (serverless v2)\n",
        "    if isinstance(resp, dict) and \"results\" in resp and resp[\"results\"]:\n",
        "        results0 = resp[\"results\"][0] or {}\n",
        "        raw_matches = results0.get(\"matches\", [])\n",
        "        matches_out = _process_matches(raw_matches)\n",
        "        return matches_out\n",
        "\n",
        "    # Caso 2: objeto con attribute 'results' (puede ser None)\n",
        "    if hasattr(resp, \"results\") and resp.results:\n",
        "        # resp.results puede ser lista-like; intentar extraer matches del primer elemento\n",
        "        try:\n",
        "            first = resp.results[0]\n",
        "            # first puede ser dict-like o un objeto con .matches\n",
        "            if isinstance(first, dict):\n",
        "                raw_matches = first.get(\"matches\", [])\n",
        "            else:\n",
        "                raw_matches = getattr(first, \"matches\", []) or (first.get(\"matches\", []) if hasattr(first, \"get\") else [])\n",
        "            matches_out = _process_matches(raw_matches)\n",
        "            return matches_out\n",
        "        except Exception:\n",
        "            # continuar a otros casos si algo raro pasa\n",
        "            pass\n",
        "\n",
        "    # Caso 3: respuesta con atributo matches directo\n",
        "    if hasattr(resp, \"matches\") and getattr(resp, \"matches\") is not None:\n",
        "        raw_matches = getattr(resp, \"matches\")\n",
        "        matches_out = _process_matches(raw_matches)\n",
        "        return matches_out\n",
        "\n",
        "    # Caso 4: dict con 'matches' en la raíz\n",
        "    if isinstance(resp, dict) and \"matches\" in resp:\n",
        "        raw_matches = resp.get(\"matches\", [])\n",
        "        matches_out = _process_matches(raw_matches)\n",
        "        return matches_out\n",
        "\n",
        "    # Fallback: devolver la respuesta cruda para inspección si no reconocemos la estructura\n",
        "    return [{\"raw_response\": resp}]"
      ],
      "metadata": {
        "id": "_jj47s4uUHjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de uso (ejecutalo tras definir la función):"
      ],
      "metadata": {
        "id": "tHLPVwweij_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = search_and_fetch(\"¿Cuál es la capital de Francia?\", top_k=3)\n",
        "for r in results:\n",
        "    print(r.get(\"id\"), r.get(\"score\"), r.get(\"metadata\", {}).get(\"text\"))\n"
      ],
      "metadata": {
        "id": "kaco7D9yinmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 4: Creación de un agente con memoria utilizando herramientas personalizadas y el patrón ReAct"
      ],
      "metadata": {
        "id": "ZjX3xD73fe9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar un agente de memoria que pueda guardar información (memorias) y consultarla a través de dos herramientas personalizadas.\n",
        "\n",
        "1. Define una herramienta add_memory(query: str) decorada con @tool, cuya función sea:\n",
        "    - Llamar a process_and_upsert_files(query) para guardar información en la base vectorial.\n",
        "    - Retornar un mensaje de éxito (“Memory Saved successfully”) o, en caso de error, un mensaje de fallo.\n",
        "\n",
        "2. Define una herramienta search_memory(query: str) decorada con @tool, que:\n",
        "    - Utilice la función search_and_fetch(query_text=query) para recuperar información.\n",
        "    - Devuelva los resultados obtenidos.\n",
        "\n",
        "3. Crea un mensaje de sistema (prompt) llamado retriever_agent_prompt que describa el propósito del agente.\n",
        "\n",
        "4. Define una lista de herramientas que contenga ambas funciones (add_memory y search_memory).\n",
        "\n",
        "5. Crea el agente de memoria usando el patrón ReAct, mediante create_react_agent(), pasando como parámetros:\n",
        "    - El modelo de lenguaje (llm_model)\n",
        "    - Las herramientas (ToolNode(tools_list))\n",
        "    - Un nombre identificativo (\"Memory_Agent\")\n",
        "    - El prompt de sistema (retriever_agent_prompt)"
      ],
      "metadata": {
        "id": "DGMMXSVDLFcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt.chat_agent_executor import create_react_agent\n",
        "from langgraph.prebuilt import ToolNode\n",
        "import traceback\n",
        "\n",
        "# asegurar llm_model\n",
        "llm_model = globals().get(\"llm_model\") or globals().get(\"chat\")\n",
        "if llm_model is None:\n",
        "    raise RuntimeError(\"No se encontró 'llm_model' ni 'chat'. Inicializalo antes de crear el agente.\")\n",
        "\n",
        "def _norm_text(t: str) -> str:\n",
        "    \"\"\"Normaliza texto para comparación simple (minúsculas, espacios).\"\"\"\n",
        "    return \" \".join(t.strip().lower().split())\n",
        "\n",
        "@tool\n",
        "def add_memory(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Guarda `query` en la base vectorial si no existe ya.\n",
        "    - Comprueba duplicados con search_and_fetch(query, top_k=10) comparando texto normalizado exacto.\n",
        "    - Si ya existe un fragmento con el mismo texto normalizado, retorna 'Memory already exists'.\n",
        "    - Si no existe, llama a process_and_upsert_files(query) y retorna 'Memory Saved successfully' o un mensaje de fallo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1) comprobar duplicado exacto normalizado\n",
        "        try:\n",
        "            resultados = search_and_fetch(query, top_k=10) or []\n",
        "        except Exception as e:\n",
        "            # si la comprobación falla, imprimimos y seguimos para intentar agregar (evitar bloqueo por error temporal)\n",
        "            print(\"add_memory: warning en search_and_fetch al comprobar duplicado:\", e)\n",
        "            traceback.print_exc()\n",
        "            resultados = []\n",
        "\n",
        "        qnorm = _norm_text(query)\n",
        "        for r in resultados:\n",
        "            meta = r.get(\"metadata\") or {}\n",
        "            candidate_text = meta.get(\"text\") or meta.get(\"snippet\") or \"\"\n",
        "            if _norm_text(candidate_text) == qnorm:\n",
        "                return \"Memory already exists\"\n",
        "\n",
        "        # 2) no hay duplicado -> insertar\n",
        "        process_and_upsert_files(query)\n",
        "        return \"Memory Saved successfully\"\n",
        "    except Exception as e:\n",
        "        # imprimir traceback para diagnóstico (no silenciamos)\n",
        "        print(\"add_memory - error al guardar memory:\", e)\n",
        "        traceback.print_exc()\n",
        "        return f\"Failed to save memory: {e}\"\n",
        "\n",
        "@tool\n",
        "def search_memory(query: str):\n",
        "    \"\"\"\n",
        "    Recupera memorias relevantes para `query` usando search_and_fetch y devuelve la lista resultante.\n",
        "    \"\"\"\n",
        "    # no capturamos excepciones aquí: el agente las verá si ocurren\n",
        "    return search_and_fetch(query_text=query, top_k=5)\n",
        "\n",
        "# Prompt del sistema\n",
        "retriever_agent_prompt = (\n",
        "    \"Eres Memory_Agent. Tu trabajo es almacenar y recuperar memorias del usuario.\\n\"\n",
        "    \"Dispones de dos herramientas:\\n\"\n",
        "    \" - add_memory(query: str): guarda una memoria (usa process_and_upsert_files). Debe evitar duplicados exactos.\\n\"\n",
        "    \" - search_memory(query: str): recupera memorias relevantes (usa search_and_fetch).\\n\\n\"\n",
        "    \"Instrucciones:\\n\"\n",
        "    \" - Si el usuario pide guardar algo, usa add_memory y confirma con 'Memory Saved successfully' o 'Memory already exists'.\\n\"\n",
        "    \" - Si el usuario pide recordar o consultar algo, usa search_memory y resume los resultados indicando las fuentes (metadata.text).\\n\"\n",
        ")\n",
        "\n",
        "# Reconstruir herramientas y agente\n",
        "tools_list = [add_memory, search_memory]\n",
        "tools_node = ToolNode(tools_list)\n",
        "\n",
        "# crear/recrear el agente (firma con prompt= en tu entorno)\n",
        "memory_agent = create_react_agent(llm_model, tools_node, prompt=retriever_agent_prompt, name=\"Memory_Agent\")\n",
        "\n",
        "# Ensure the name attribute exists and is correct for the supervisor\n",
        "if not hasattr(memory_agent, 'name') or memory_agent.name is None or memory_agent.name == \"LangGraph\":\n",
        "    print(f\"Warning: Setting 'name' attribute on memory_agent to 'Memory_Agent'.\")\n",
        "    memory_agent.name = \"Memory_Agent\"\n",
        "\n",
        "\n",
        "print(\"Tools actualizadas y agente 'Memory_Agent' recreado correctamente.\")"
      ],
      "metadata": {
        "id": "YK_s65ffLMxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de uso (ejecutalo tras definir la función):"
      ],
      "metadata": {
        "id": "hy9cMFZ2IJAt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd1be436"
      },
      "source": [
        "import time\n",
        "import traceback\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "def call_agent(agent, prompt: str):\n",
        "    state_in = { \"messages\": [HumanMessage(content=prompt)] }\n",
        "    result_state = agent.invoke(state_in)\n",
        "\n",
        "    msgs = result_state[\"messages\"]\n",
        "\n",
        "    # capturar tool usada\n",
        "    tools_used = [m for m in msgs if isinstance(m, ToolMessage)]\n",
        "    tool_name = tools_used[-1].name if tools_used else None\n",
        "\n",
        "    # respuesta del agente\n",
        "    ai_msgs = [m for m in msgs if isinstance(m, AIMessage)]\n",
        "    output = ai_msgs[-1].content if ai_msgs else msgs[-1].content\n",
        "\n",
        "    return {\n",
        "        \"response\": output,\n",
        "        \"tool_used\": tool_name\n",
        "    }\n",
        "\n",
        "\n",
        "# memorias de prueba\n",
        "memorias_prueba = [\n",
        "    \"La capital de Francia es Paris.\",\n",
        "    \"Trabajo en IA y me interesa NLP y sistemas RAG.\",\n",
        "    \"Me gusta el café por la mañana y suelo tomar un espresso doble.\"\n",
        "]\n",
        "\n",
        "print(\"=== PRUEBAS SOBRE AGENTE Memory_Agent (con deduplicado en add_memory) ===\\n\")\n",
        "\n",
        "# 1) Guardar memorias a través del agente\n",
        "print(\"1) Guardando memorias mediante el agente:\")\n",
        "for texto in memorias_prueba:\n",
        "    try:\n",
        "        prompt = f\"Guarda: {texto}\"\n",
        "        out = call_agent(memory_agent, prompt)\n",
        "        print(f\" - Prompt: {prompt}\")\n",
        "        print(\"   Agent ->\", out)\n",
        "        time.sleep(0.2)\n",
        "    except Exception:\n",
        "        print(\"   Error invocando agente para guardar (ver traceback):\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# 2) Intentar guardar duplicados (misma consulta exacta)\n",
        "print(\"\\n2) Intentando guardar duplicados (debe informar 'Memory already exists'):\")\n",
        "for texto in memorias_prueba:\n",
        "    try:\n",
        "        prompt = f\"Guarda: {texto}\"  # misma consulta\n",
        "        out = call_agent(memory_agent, prompt)\n",
        "        print(f\" - Prompt: {prompt}\")\n",
        "        print(\"   Agent ->\", out)\n",
        "        time.sleep(0.1)\n",
        "    except Exception:\n",
        "        print(\"   Error invocando agente para duplicado (ver traceback):\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# 3) Recuperar con prompts naturales (el agente debe usar search_memory)\n",
        "print(\"\\n3) Recuperación usando el agente (prompts naturales):\")\n",
        "queries = [\n",
        "    \"¿Cuál es la capital de Francia?\",\n",
        "    \"¿Qué dije sobre mi trabajo en IA?\",\n",
        "    \"¿Qué dije sobre el café?\"\n",
        "]\n",
        "for q in queries:\n",
        "    try:\n",
        "        print(f\"\\n - Prompt agente: {q}\")\n",
        "        agent_resp = call_agent(memory_agent, q)\n",
        "        print(\"   Agent ->\", agent_resp)\n",
        "        # Mostrar low-level matches para verificación\n",
        "        try:\n",
        "            low = search_and_fetch(q, top_k=5)\n",
        "            print(\"   Low-level matches:\")\n",
        "            if not low:\n",
        "                print(\"    (sin resultados)\")\n",
        "            else:\n",
        "                for i, r in enumerate(low, 1):\n",
        "                    meta = r.get(\"metadata\") or {}\n",
        "                    text = meta.get(\"text\") or meta.get(\"snippet\") or \"<sin text>\"\n",
        "                    print(f\"    {i}) score={r.get('score')} | id={r.get('id')} | text: {text}\")\n",
        "        except Exception:\n",
        "            print(\"   Error en search_and_fetch (ver traceback):\")\n",
        "            traceback.print_exc()\n",
        "    except Exception:\n",
        "        print(\"   Error invocando agente (ver traceback):\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n=== FIN PRUEBAS AGENTE Memory_Agent ===\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 5: Creación de un agente de investigación con la API de Wikipedia y el patrón ReAct"
      ],
      "metadata": {
        "id": "WBmpy-ltfxMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar un agente de búsqueda que utilice la API de Wikipedia para responder preguntas o investigar temas de interés.\n",
        "1. Crea una instancia del conector de Wikipedia y define la herramienta de búsqueda.\n",
        "2. Crea un mensaje de sistema (prompt) que describa el rol del agente.\n",
        "3. Agrupa las herramientas en una lista.\n",
        "4. Crea el agente de investigación (Research Agent) utilizando el patrón ReAct, mediante la función create_react_agent()."
      ],
      "metadata": {
        "id": "riATPNS8LI6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langgraph.prebuilt.chat_agent_executor import create_react_agent\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# Aseguramos que exista el modelo de lenguaje\n",
        "llm_model = globals().get(\"llm_model\") or globals().get(\"chat\")\n",
        "if llm_model is None:\n",
        "    raise RuntimeError(\"No se encontró 'llm_model' ni 'chat'. Inicialízalo antes de crear el Research_Agent.\")\n",
        "\n",
        "# Conector de Wikipedia (en español, puedes cambiar lang=\"es\" si lo prefieres en otro idioma)\n",
        "api_wrapper = WikipediaAPIWrapper(lang=\"es\", doc_content_chars_max=2000)\n",
        "\n",
        "# Herramienta de búsqueda en Wikipedia\n",
        "wikipedia_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "\n",
        "# Prompt del sistema para el agente de investigación\n",
        "search_agent_prompt = (\n",
        "    \"Eres Research_Agent. Tu objetivo es investigar temas utilizando la API de Wikipedia.\\n\"\n",
        "    \"Dispones de una herramienta llamada 'wikipedia' que consulta Wikipedia a partir de una consulta de texto.\\n\\n\"\n",
        "    \"Instrucciones:\\n\"\n",
        "    \" - Cuando el usuario haga una pregunta sobre un concepto, evento, persona o tema de interés, \"\n",
        "    \"   usa la herramienta de Wikipedia para obtener información.\\n\"\n",
        "    \" - Integra y resume la información encontrada y responde de forma clara, concisa y en español.\\n\"\n",
        "    \" - Cita brevemente las secciones relevantes cuando sea útil para el usuario.\\n\"\n",
        ")\n",
        "\n",
        "# Lista de herramientas del agente de investigación\n",
        "tools_list = [wikipedia_tool]\n",
        "tools_node = ToolNode(tools_list)\n",
        "\n",
        "# Creación del agente de investigación usando el patrón ReAct\n",
        "research_agent = create_react_agent(\n",
        "    llm_model,\n",
        "    tools_node,\n",
        "    prompt=search_agent_prompt,\n",
        "    name=\"Research_Agent\"\n",
        ")\n",
        "\n",
        "# Ensure the name attribute exists and is correct for the supervisor\n",
        "if not hasattr(research_agent, 'name') or research_agent.name is None or research_agent.name == \"LangGraph\":\n",
        "    print(f\"Warning: Setting 'name' attribute on research_agent to 'Research_Agent'.\")\n",
        "    research_agent.name = \"Research_Agent\"\n",
        "\n",
        "\n",
        "print(\"Agente 'Research_Agent' creado correctamente.\")"
      ],
      "metadata": {
        "id": "muYdtPmCLNGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de uso (ejecutalo tras definir la función):"
      ],
      "metadata": {
        "id": "i3ltC65NKjcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "import time, traceback\n",
        "\n",
        "print(\"=== PRUEBAS SOBRE Research_Agent (Wikipedia) ===\\n\")\n",
        "\n",
        "queries = [\n",
        "    \"¿Quién fue Alan Turing?\",\n",
        "    \"Explica brevemente qué es el aprendizaje profundo.\",\n",
        "    \"¿Qué es la teoría de la relatividad?\",\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(f\"Pregunta: {q}\")\n",
        "    try:\n",
        "        state_in = {\"messages\": [HumanMessage(content=q)]}\n",
        "        result_state = research_agent.invoke(state_in)\n",
        "\n",
        "        msgs = result_state[\"messages\"]\n",
        "        ai_msgs = [m for m in msgs if isinstance(m, AIMessage)]\n",
        "        answer = ai_msgs[-1].content if ai_msgs else msgs[-1].content\n",
        "\n",
        "        print(f\"Respuesta del agente:\\n\\n{answer}\")\n",
        "    except Exception:\n",
        "        print(\"  Error invocando Research_Agent (ver traceback):\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "    time.sleep(0.2)\n",
        "\n",
        "print(\"=== FIN PRUEBAS Research_Agent ===\")"
      ],
      "metadata": {
        "id": "0E089dnrJeen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 6: Creación de un agente supervisor para coordinar los agentes de memoria e investigación"
      ],
      "metadata": {
        "id": "vZwRJtIcf4g2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar un agente supervisor que dirija dinámicamente las solicitudes del usuario hacia el agente más adecuado (ya sea el de memoria o el de investigación).\n",
        "1. Define el prompt del supervisor, que describa su rol de coordinación\n",
        "2. Crea el supervisor utilizando la función create_supervisor(), especificando:\n",
        "    - La lista de agentes que supervisará: [research_agent, memory_agent].\n",
        "    - El modelo de lenguaje (llm_model).\n",
        "    - El prompt del sistema (supervisor_agent_prompt).\n",
        "    - El modo de salida (output_mode=\"full_history\") para conservar el registro completo de la interacción.\n",
        "3. Compila el agente supervisor.\n"
      ],
      "metadata": {
        "id": "6wLBGWVAMiO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph_supervisor import create_supervisor\n",
        "\n",
        "# Prompt del supervisor: coordina Memory_Agent y Research_Agent\n",
        "supervisor_agent_prompt = (\n",
        "    \"Eres un agente supervisor que coordina dos agentes especializados:\\n\"\n",
        "    \"- Memory_Agent: gestiona memorias personales del usuario (guardar y recuperar información propia).\\n\"\n",
        "    \"- Research_Agent: investiga temas generales utilizando Wikipedia.\\n\\n\"\n",
        "    \"Instrucciones:\\n\"\n",
        "    \"- Si la consulta del usuario se refiere a información personal, recuerdos, notas o algo que debería guardarse \"\n",
        "    \"  o recuperarse más tarde, delega en Memory_Agent.\\n\"\n",
        "    \"- Si la consulta requiere conocimiento enciclopédico, hechos históricos, definiciones o investigación general, \"\n",
        "    \"  delega en Research_Agent.\\n\"\n",
        "    \"- Asigna trabajo a un solo agente por turno y no hagas tú mismo el trabajo; tu rol es sólo decidir y orquestar.\\n\"\n",
        "    \"- Devuelve siempre al usuario una respuesta clara y concisa en español, basada en el resultado del agente elegido.\\n\\n\"\n",
        "    # ---- Instrucción añadida para testing automático ----\n",
        "    \"IMPORTANTE PARA TESTS AUTOMATIZADOS: además de la respuesta natural al usuario, \"\n",
        "    \"al final añade en una línea separada un JSON válido con los campos \"\n",
        "    \"`delegated_agent` y `tool_used`. Ejemplo:\\n\"\n",
        "    '{\"delegated_agent\": \"Memory_Agent\", \"tool_used\": \"memory.save\"}\\n'\n",
        "    \"Este JSON debe ser en una sola línea y válido para parsear por el test.\\n\"\n",
        "    # ----------------------------------------------------\n",
        ")\n",
        "\n",
        "\n",
        "# Crear el supervisor (workflow) que coordina research_agent y memory_agent\n",
        "supervisor = create_supervisor(\n",
        "    [research_agent, memory_agent],\n",
        "    model=llm_model,\n",
        "    prompt=supervisor_agent_prompt,\n",
        "    output_mode=\"full_history\",  # conservar todo el historial de la interacción\n",
        ")\n",
        "\n",
        "# Compilar el agente supervisor listo para invocarse\n",
        "supervisor_agent = supervisor.compile(name=\"Supervisor_Agent\")"
      ],
      "metadata": {
        "id": "uwVBmmoSMjcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de uso (ejecutalo tras definir la función):"
      ],
      "metadata": {
        "id": "2daC2iF4MTHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import traceback\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "print(\"=== PRUEBAS SOBRE Supervisor_Agent ===\\n\")\n",
        "\n",
        "# Casos de prueba: algunos deberían ir a Memory_Agent y otros a Research_Agent\n",
        "pruebas = [\n",
        "    (\"Memoria - guardar\",   \"Guarda: Mi película favorita es Matrix.\"),\n",
        "    (\"Memoria - recuperar\", \"¿Qué dije sobre mi película favorita?\"),\n",
        "    (\"Investigación\",       \"¿Quién fue Alan Turing?\"),\n",
        "    (\"Investigación\",       \"Explica brevemente qué es el aprendizaje profundo.\"),\n",
        "]\n",
        "\n",
        "for etiqueta, prompt in pruebas:\n",
        "    print(f\"[Caso: {etiqueta}]\")\n",
        "    print(f\"Prompt del usuario: {prompt}\")\n",
        "    try:\n",
        "        # Estado inicial para el supervisor\n",
        "        state_in = {\n",
        "            \"messages\": [HumanMessage(content=prompt)]\n",
        "        }\n",
        "\n",
        "        # Invocar al agente supervisor\n",
        "        result_state = supervisor_agent.invoke(state_in)\n",
        "\n",
        "        # El supervisor fue creado con output_mode=\"full_history\",\n",
        "        # así que result_state[\"messages\"] incluye toda la conversación\n",
        "        msgs = result_state[\"messages\"]\n",
        "\n",
        "        # 1) detectar qué agente se invocó (último AIMessage)\n",
        "        ai_msgs = [m for m in msgs if isinstance(m, AIMessage)]\n",
        "        if ai_msgs:\n",
        "            last_ai = ai_msgs[-1]\n",
        "        else:\n",
        "            last_ai = msgs[-1]\n",
        "\n",
        "        # Nombre del agente que generó la respuesta (si está disponible)\n",
        "        agent_name = getattr(last_ai, \"name\", \"desconocido\")\n",
        "\n",
        "        # 2) detectar qué herramienta se ejecutó (último ToolMessage)\n",
        "        tool_msgs = [m for m in msgs if isinstance(m, ToolMessage)]\n",
        "        last_tool = tool_msgs[-1].name if tool_msgs else \"ninguna\"\n",
        "\n",
        "        print(f\"Agente delegado: {agent_name}\")\n",
        "        print(f\"Herramienta usada: {last_tool}\")\n",
        "        print(\"Respuesta del agente:\\n\" + last_ai.content)\n",
        "\n",
        "    except Exception:\n",
        "        print(\"  Error invocando Supervisor_Agent (ver traceback):\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "    time.sleep(0.2)\n",
        "\n",
        "print(\"=== FIN PRUEBAS Supervisor_Agent ===\")"
      ],
      "metadata": {
        "id": "l1ARjuCQMT4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 7: Construcción del grafo de orquestación con LangGraph"
      ],
      "metadata": {
        "id": "RcbMcM_7f-VN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diseñar y compilar un grafo funcional que orqueste la interacción entre el supervisor y los agentes subordinados (investigador y memoria), utilizando LangGraph.\n",
        "1. Crea una instancia del grafo\n",
        "2. Agrega los nodos y aristas correspondientes\n",
        "3. Compila el grafo final"
      ],
      "metadata": {
        "id": "_HBb_SSgWGpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "from langgraph.graph import add_messages\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "import re # Import the re module for regular expressions\n",
        "\n",
        "# 1. Crea una instancia del grafo\n",
        "# Define el estado del grafo. En este caso, solo necesitamos el historial de mensajes.\n",
        "# LangGraph ya tiene un tipo de estado predefinido para esto: MessagesState\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "# 2. Agrega los nodos y aristas correspondientes\n",
        "\n",
        "# Nodos para los agentes y el supervisor\n",
        "workflow.add_node(\"supervisor\", supervisor_agent)\n",
        "workflow.add_node(\"research_agent\", research_agent)\n",
        "workflow.add_node(\"memory_agent\", memory_agent)\n",
        "\n",
        "# Define el punto de entrada\n",
        "workflow.add_edge(START, \"supervisor\")\n",
        "\n",
        "# Define la función de enrutamiento basada en la decisión del supervisor\n",
        "# El supervisor debe devolver un nombre de agente válido (\"research_agent\" o \"memory_agent\")\n",
        "def route_agents(state):\n",
        "    # El último mensaje debe ser del supervisor y contener la decisión.\n",
        "    decision_raw = state[\"messages\"][-1].content.strip()\n",
        "    print(f\"Supervisor decidió (raw): {decision_raw}\")\n",
        "\n",
        "    # Try to extract the agent name by looking for specific patterns in the output\n",
        "    decision = None\n",
        "    match = re.search(r\"Successfully transferred to (Memory_Agent|Research_Agent)\", decision_raw)\n",
        "    if match:\n",
        "        decision = match.group(1).lower() # Extract the agent name and convert to lowercase\n",
        "\n",
        "    if decision is None:\n",
        "        # Fallback or error handling if decision is unclear\n",
        "        print(f\"Advertencia: Decisión del supervisor no clara o formato inesperado: '{decision_raw}'. Volviendo al supervisor.\")\n",
        "        decision = \"supervisor\" # Or handle as an error/default\n",
        "\n",
        "    print(f\"Supervisor decidió (enrutado): {decision}\")\n",
        "    return decision\n",
        "\n",
        "# Agrega la arista condicional desde el supervisor\n",
        "# La arista va del supervisor a la función de enrutamiento.\n",
        "# La función de enrutamiento decide a dónde ir a continuación.\n",
        "workflow.add_conditional_edges(\n",
        "    \"supervisor\", # Desde el nodo supervisor\n",
        "    route_agents, # Función que decide a dónde ir\n",
        "    {\n",
        "        \"research_agent\": \"research_agent\", # Si route_agents devuelve \"research_agent\", ir al nodo research_agent\n",
        "        \"memory_agent\": \"memory_agent\",   # Si route_agents devuelve \"memory_agent\", ir al nodo memory_agent\n",
        "        \"supervisor\": \"supervisor\" # Added fallback to supervisor\n",
        "    }\n",
        ")\n",
        "\n",
        "# Los agentes (memory_agent y research_agent) siempre terminan su ejecución\n",
        "# y devuelven el control al supervisor (o al final del grafo, dependiendo del diseño)\n",
        "# Para este ejemplo simple, asumimos que después de que un agente responde, el proceso termina.\n",
        "workflow.add_edge(\"research_agent\", END)\n",
        "workflow.add_edge(\"memory_agent\", END)\n",
        "\n",
        "\n",
        "# 3. Compila el grafo final\n",
        "graph = workflow.compile()\n",
        "\n",
        "print(\"Grafo funcional compilado correctamente.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WKRrW8i6NN4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruebas para validar grafo\n",
        "\n",
        "- Modo conciso: imprime solo la etiqueta del caso, el agente delegado (si se detecta) y la respuesta final:\n",
        "\n"
      ],
      "metadata": {
        "id": "wM1XMdE5W9j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, json, re, traceback\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "pruebas = [\n",
        "    (\"Memoria - guardar\",   \"Guarda: Mi película favorita es Matrix.\"),\n",
        "    (\"Memoria - recuperar\", \"¿Qué dije sobre mi película favorita?\"),\n",
        "    (\"Investigación\",       \"¿Quién fue Alan Turing?\"),\n",
        "    (\"Investigación\",       \"Explica brevemente qué es el aprendizaje profundo.\"),\n",
        "]\n",
        "\n",
        "# patrón para detectar mensajes de hand-off cortos (evitarlos)\n",
        "handoff_re = re.compile(r'transfer(back|ring)?|transferri?ng back|transferido al supervisor', re.IGNORECASE)\n",
        "\n",
        "def find_meta(msgs):\n",
        "    for m in reversed(msgs):\n",
        "        if isinstance(m, AIMessage):\n",
        "            j = re.search(r'(\\{(?:.|\\s)*\\})\\s*$', (m.content or \"\").strip(), re.DOTALL)\n",
        "            if j:\n",
        "                try:\n",
        "                    meta = json.loads(j.group(1))\n",
        "                    if isinstance(meta, dict) and \"delegated_agent\" in meta:\n",
        "                        return meta\n",
        "                except Exception:\n",
        "                    pass\n",
        "    return None\n",
        "\n",
        "def select_response(msgs):\n",
        "    # 1) si hay metadata con delegated_agent -> elegir AIMessage de ese agente más representativo\n",
        "    meta = find_meta(msgs)\n",
        "    if meta and meta.get(\"delegated_agent\"):\n",
        "        delegated = meta[\"delegated_agent\"].lower()\n",
        "        candidates = [(i, m) for i, m in enumerate(msgs)\n",
        "                      if isinstance(m, AIMessage) and (getattr(m, \"name\", \"\") or \"\").lower() == delegated]\n",
        "        if candidates:\n",
        "            filtered = [(i,m) for i,m in candidates if not (len((m.content or \"\").strip())<80 and handoff_re.search(m.content or \"\"))]\n",
        "            pool = filtered or candidates\n",
        "            idx, chosen = max(pool, key=lambda im: len((im[1].content or \"\")))\n",
        "            return chosen, meta.get(\"delegated_agent\")\n",
        "    # 2) si no hay metadata, escoger AIMessage con nombre de agente conocido (si existe) o el AIMessage más largo\n",
        "    known_agent_names_lower = {\"memory_agent\", \"memory_agent\".lower(), \"research_agent\", \"research_agent\".lower(),\n",
        "                               \"memory_agent\", \"research_agent\", \"supervisor\", \"supervisor_agent\"}\n",
        "    ai_msgs = [(i, m) for i, m in enumerate(msgs) if isinstance(m, AIMessage)]\n",
        "    by_name = [(i,m) for i,m in ai_msgs if (getattr(m, \"name\", \"\") or \"\").lower() in known_agent_names_lower]\n",
        "    if by_name:\n",
        "        idx, chosen = max(by_name, key=lambda im: len((im[1].content or \"\")))\n",
        "        return chosen, getattr(chosen, \"name\", None)\n",
        "    if ai_msgs:\n",
        "        idx, chosen = max(ai_msgs, key=lambda im: len((im[1].content or \"\")))\n",
        "        return chosen, getattr(chosen, \"name\", None)\n",
        "    return None, None\n",
        "\n",
        "for etiqueta, prompt in pruebas:\n",
        "    try:\n",
        "        state_in = {\"messages\": [HumanMessage(content=prompt)]}\n",
        "        invoker = supervisor_agent if 'supervisor_agent' in globals() else (graph if 'graph' in globals() else None)\n",
        "        if invoker is None:\n",
        "            raise RuntimeError(\"No se encontró 'supervisor_agent' ni 'graph' en el entorno.\")\n",
        "        result = invoker.invoke(state_in)\n",
        "        msgs = result.get(\"messages\", [])\n",
        "        chosen, agent = select_response(msgs)\n",
        "        respuesta = (chosen.content or \"\").strip() if chosen else \"(sin respuesta identificada)\"\n",
        "        agente_label = agent or \"desconocido\"\n",
        "        print(f\"[{etiqueta}]\")\n",
        "        print(f\"Agente delegado: {agente_label}\")\n",
        "        print(\"Respuesta:\\n\" + respuesta + \"\\n\")\n",
        "    except Exception:\n",
        "        print(f\"[{etiqueta}] Error ejecutando la prueba:\")\n",
        "        traceback.print_exc()\n",
        "    time.sleep(0.12)\n"
      ],
      "metadata": {
        "id": "vmKBL9luXDG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Modo de debug: imprime historial, metadata, motivo de selección y el mensaje completo del agente delegado:"
      ],
      "metadata": {
        "id": "YvJVYxcdWHB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import traceback\n",
        "import json\n",
        "import re\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "print(\"=== PRUEBAS SOBRE Supervisor_Agent (mejoradas, corrección selección Research_Agent) ===\\n\")\n",
        "\n",
        "# Casos de prueba\n",
        "pruebas = [\n",
        "    (\"Memoria - guardar\",   \"Guarda: Mi película favorita es Matrix.\"),\n",
        "    (\"Memoria - recuperar\", \"¿Qué dije sobre mi película favorita?\"),\n",
        "    (\"Investigación\",       \"¿Quién fue Alan Turing?\"),\n",
        "    (\"Investigación\",       \"Explica brevemente qué es el aprendizaje profundo.\"),\n",
        "]\n",
        "\n",
        "# Detectar nombres conocidos (intenta leer .name si existe)\n",
        "known_agent_names = set()\n",
        "try:\n",
        "    if 'memory_agent' in globals() and hasattr(memory_agent, 'name'):\n",
        "        known_agent_names.add(getattr(memory_agent, 'name'))\n",
        "    if 'research_agent' in globals() and hasattr(research_agent, 'name'):\n",
        "        known_agent_names.add(getattr(research_agent, 'name'))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "known_agent_names.update({\n",
        "    \"Memory_Agent\", \"memory_agent\", \"MemoryAgent\", \"memoryagent\",\n",
        "    \"Research_Agent\", \"research_agent\", \"ResearchAgent\", \"researchagent\",\n",
        "    \"Supervisor_Agent\", \"supervisor\", \"Supervisor\"\n",
        "})\n",
        "known_agent_names_lower = {n.lower() for n in known_agent_names}\n",
        "\n",
        "def dump_history(msgs):\n",
        "    print(\">> Historial completo (índice, tipo, name/role, len):\")\n",
        "    for i, m in enumerate(msgs):\n",
        "        typ = type(m).__name__\n",
        "        name = getattr(m, \"name\", getattr(m, \"role\", \"<sin nombre>\"))\n",
        "        content = getattr(m, \"content\", \"\")\n",
        "        snippet = content if len(content) <= 220 else content[:220] + \"...(truncado)\"\n",
        "        print(f\"  [{i:02d}] {typ:12} | name/role='{name}' | len={len(content):4} | snippet: {snippet!r}\")\n",
        "    print(\"\")\n",
        "\n",
        "def find_json_metadata_in_messages(msgs):\n",
        "    for i, m in reversed(list(enumerate(msgs))):\n",
        "        if isinstance(m, AIMessage):\n",
        "            text = m.content.strip()\n",
        "            jmatch = re.search(r'(\\{(?:.|\\s)*\\})\\s*$', text)\n",
        "            if jmatch:\n",
        "                try:\n",
        "                    meta = json.loads(jmatch.group(1))\n",
        "                    if isinstance(meta, dict) and (\"delegated_agent\" in meta or \"tool_used\" in meta):\n",
        "                        return meta, i, m\n",
        "                except Exception:\n",
        "                    continue\n",
        "    return None, None, None\n",
        "\n",
        "# patrón para detectar mensajes de hand-off cortos (en varios idiomas/frases comunes)\n",
        "handoff_pattern = re.compile(r'(transfer(back|ring)?\\s*(to|back)?\\s*supervisor|transferri?ng back to supervisor|successfully transferred back|transferido al supervisor)', re.IGNORECASE)\n",
        "\n",
        "def pick_agent_response(msgs):\n",
        "    \"\"\"\n",
        "    Mejor lógica:\n",
        "    - Si hay JSON metadata -> buscar TODOS los AIMessage con name==delegated_agent y elegir el más largo\n",
        "      (ignorar mensajes cortos / de handoff mediante patrón)\n",
        "    - Si no hay metadata, aplicar heurísticas previas (por nombre conocido, tool messages, o AI más largo).\n",
        "    Devuelve: (chosen_message, chosen_index, reason, meta)\n",
        "    \"\"\"\n",
        "    meta, meta_idx, meta_msg = find_json_metadata_in_messages(msgs)\n",
        "    if meta:\n",
        "        delegated = meta.get(\"delegated_agent\")\n",
        "        tool_used = meta.get(\"tool_used\")\n",
        "        reason = f\"Encontrado JSON metadata en AIMessage índice {meta_idx} (delegated_agent={delegated}, tool_used={tool_used})\"\n",
        "\n",
        "        # Buscar todas las AIMessage cuyo .name coincide con delegated (case-insensitive)\n",
        "        candidates = []\n",
        "        if delegated:\n",
        "            for i, m in enumerate(msgs):\n",
        "                if isinstance(m, AIMessage) and getattr(m, \"name\", \"\").lower() == delegated.lower():\n",
        "                    candidates.append((i, m))\n",
        "\n",
        "            if candidates:\n",
        "                # Filtrar mensajes claramente de handoff o extremadamente cortos\n",
        "                filtered = []\n",
        "                for i, m in candidates:\n",
        "                    text = (m.content or \"\").strip()\n",
        "                    if len(text) < 50 and handoff_pattern.search(text):\n",
        "                        # descartamos evidentes handoffs cortos\n",
        "                        continue\n",
        "                    filtered.append((i, m))\n",
        "\n",
        "                # Si el filtrado deja vacíos, usamos los candidatos originales (para no perder todo)\n",
        "                pool = filtered if filtered else candidates\n",
        "\n",
        "                # Elegir el de mayor longitud de contenido\n",
        "                chosen_idx, chosen_msg = max(pool, key=lambda im: len((im[1].content or \"\")))\n",
        "                reason += f\" -> {len(pool)} candidato(s) encontrados, seleccionado índice {chosen_idx} por mayor longitud\"\n",
        "                return chosen_msg, chosen_idx, reason, meta\n",
        "\n",
        "            else:\n",
        "                reason += \" -> NO se encontraron AIMessage con el nombre delegado en el historial.\"\n",
        "                # continuamos con heurísticas fallback\n",
        "\n",
        "    # ---------------- Fallbacks (sin metadata o metadata inconclusa) ----------------\n",
        "    # 1) AIMessages con nombre de agente conocido (preferir más largos)\n",
        "    ai_msgs = [(i, m) for i, m in enumerate(msgs) if isinstance(m, AIMessage)]\n",
        "    candidates_by_name = [(i, m) for i, m in ai_msgs if getattr(m, \"name\", \"\").lower() in known_agent_names_lower]\n",
        "    if candidates_by_name:\n",
        "        chosen_idx, chosen_msg = max(candidates_by_name, key=lambda im: len((im[1].content or \"\")))\n",
        "        reason = \"Cruce por nombre: elegido AIMessage (agente conocido) con mayor longitud\"\n",
        "        return chosen_msg, chosen_idx, reason, meta\n",
        "\n",
        "    # 2) ToolMessage significativa -> buscar AIMessage cerca\n",
        "    tool_msgs = [(i, m) for i, m in enumerate(msgs) if isinstance(m, ToolMessage)]\n",
        "    meaningful_tools = [(i, m) for i, m in tool_msgs if m.name and \"transfer_back_to_supervisor\" not in m.name.lower()]\n",
        "    if meaningful_tools:\n",
        "        idx, tool_m = meaningful_tools[-1]\n",
        "        after_msgs = [(i, m) for i, m in enumerate(msgs[idx+1:], start=idx+1) if isinstance(m, AIMessage)]\n",
        "        if after_msgs:\n",
        "            chosen_idx, chosen_msg = after_msgs[0]\n",
        "            reason = f\"Encontrada ToolMessage significativa '{tool_m.name}' (índice {idx}) -> tomando AIMessage siguiente (índice {chosen_idx})\"\n",
        "            return chosen_msg, chosen_idx, reason, meta\n",
        "        before_msgs = [(i, m) for i, m in enumerate(msgs[:idx]) if isinstance(m, AIMessage)]\n",
        "        if before_msgs:\n",
        "            chosen_idx, chosen_msg = before_msgs[-1]\n",
        "            reason = f\"Encontrada ToolMessage significativa '{tool_m.name}' (índice {idx}) -> tomando AIMessage anterior (índice {chosen_idx})\"\n",
        "            return chosen_msg, chosen_idx, reason, meta\n",
        "\n",
        "    # 3) Si hay AIMessages múltiples, preferir el más largo\n",
        "    if ai_msgs:\n",
        "        chosen_idx, chosen_msg = max(ai_msgs, key=lambda im: len((im[1].content or \"\")))\n",
        "        reason = \"Fallback: elegido AIMessage más largo del historial\"\n",
        "        return chosen_msg, chosen_idx, reason, meta\n",
        "\n",
        "    return None, None, \"No se halló AIMessage en el historial\", meta\n",
        "\n",
        "# Ejecutar pruebas\n",
        "for etiqueta, prompt in pruebas:\n",
        "    print(f\"[Caso: {etiqueta}]\")\n",
        "    print(f\"Prompt del usuario: {prompt}\")\n",
        "    try:\n",
        "        state_in = {\"messages\": [HumanMessage(content=prompt)]}\n",
        "\n",
        "        # elegir invocador (supervisor_agent o graph)\n",
        "        if 'supervisor_agent' in globals():\n",
        "            invoker = supervisor_agent\n",
        "            invoker_name = \"supervisor_agent\"\n",
        "        elif 'graph' in globals():\n",
        "            invoker = graph\n",
        "            invoker_name = \"graph\"\n",
        "        else:\n",
        "            raise RuntimeError(\"No se encontró 'supervisor_agent' ni 'graph' en el entorno.\")\n",
        "\n",
        "        print(f\"  Paso 1: invocando {invoker_name}.invoke(...)\")\n",
        "        result_state = invoker.invoke(state_in)\n",
        "        print(\"  Paso 2: invocación completada, extrayendo full_history\\n\")\n",
        "\n",
        "        msgs = result_state.get(\"messages\", [])\n",
        "        dump_history(msgs)\n",
        "\n",
        "        chosen_msg, chosen_idx, reason, meta = pick_agent_response(msgs)\n",
        "\n",
        "        # Mostrar info adicional sobre la tool wikipedia si existe\n",
        "        tool_msgs_all = [(i, m) for i, m in enumerate(msgs) if isinstance(m, ToolMessage)]\n",
        "        wiki_tool = next(((i, m) for i, m in tool_msgs_all if getattr(m, \"name\", \"\").lower() in {\"wikipedia\", \"wiki\"}), None)\n",
        "        if wiki_tool:\n",
        "            i_w, m_w = wiki_tool\n",
        "            print(f\"Se detectó ToolMessage de Wikipedia en índice {i_w}, longitud {len(m_w.content or '')} caracteres.\")\n",
        "\n",
        "        if chosen_msg:\n",
        "            agent_name = getattr(chosen_msg, \"name\", \"desconocido\")\n",
        "            content_full = getattr(chosen_msg, \"content\", \"\")\n",
        "            print(\"======== Resultado detectado ========\")\n",
        "            print(f\"Motivo de selección: {reason}\")\n",
        "            if meta:\n",
        "                print(f\"Metadata JSON detectada: {meta}\")\n",
        "            print(f\"Índice del AIMessage seleccionado: {chosen_idx}\")\n",
        "            print(f\"Agente declarado en el AIMessage seleccionado: '{agent_name}'\")\n",
        "            print(f\"Longitud de contenido seleccionada: {len(content_full)} caracteres\")\n",
        "            print(\"\\n--- Contenido completo del mensaje seleccionado ---\\n\")\n",
        "            print(content_full)\n",
        "            print(\"\\n--- fin del contenido ---\\n\")\n",
        "        else:\n",
        "            print(\"NO se pudo identificar un AIMessage representativo del agente delegado.\")\n",
        "            if tool_msgs_all:\n",
        "                last_tool = tool_msgs_all[-1][1]\n",
        "                print(f\"Última ToolMessage encontrada: name='{last_tool.name}', content_len={len(getattr(last_tool, 'content', ''))}\")\n",
        "            print(\"Historial completo arriba para debug.\")\n",
        "\n",
        "    except Exception:\n",
        "        print(\"  Error invocando Supervisor/Graph (ver traceback):\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
        "    time.sleep(0.2)\n",
        "\n",
        "print(\"=== FIN PRUEBAS Supervisor_Agent ===\")\n"
      ],
      "metadata": {
        "id": "rpEd8ZKZNcx9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}